{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkOzgziHCCyZ"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import logging\n",
        "import nest_asyncio\n",
        "\n",
        "# Define required packages\n",
        "required_packages = [\n",
        "    \"llama-index-embeddings-huggingface\",\n",
        "    \"llama-index-vector-stores-chroma\",\n",
        "    \"llama-index-retrievers-bm25\",\n",
        "    \"datasets==3.4.0\", \"chromadb==0.5.17\", \"peft==0.10.0\",\n",
        "    \"transformers==4.40.0\", \"llama-index-readers-file\", \"xformers\",\n",
        "    \"duckduckgo_search\", \"wikipedia\",\n",
        "    \"ragas\", \"langchain-groq\", \"google-generativeai\", \"langchain-google-genai\",\n",
        "    \"pdfminer.six\", \"PyMuPDF==1.25.2\"\n",
        "]\n",
        "\n",
        "# Install required packages\n",
        "def install_packages(packages):\n",
        "    for package in packages:\n",
        "        subprocess.run([\"pip\", \"install\", package], check=True)\n",
        "\n",
        "install_packages(required_packages)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import base64\n",
        "import asyncio\n",
        "import logging\n",
        "import mimetypes\n",
        "import concurrent.futures\n",
        "from random import random\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, List, Optional, Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Third-party data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PDF processing\n",
        "import fitz  # PyMuPDF\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# AI/ML APIs\n",
        "from google import genai\n",
        "import google.generativeai as generai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# Datasets and embeddings\n",
        "import datasets\n",
        "from datasets import load_dataset, Dataset\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Vector database\n",
        "import chromadb\n",
        "\n",
        "# LlamaIndex core\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    Settings,\n",
        "    StorageContext,\n",
        "    Document\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter, SimpleNodeParser\n",
        "from llama_index.core.llms import MockLLM\n",
        "from llama_index.core.retrievers import QueryFusionRetriever\n",
        "from llama_index.core.retrievers.fusion_retriever import FUSION_MODES\n",
        "\n",
        "# LlamaIndex extensions\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "\n",
        "# Search engines\n",
        "from duckduckgo_search import DDGS\n",
        "import wikipedia\n",
        "\n",
        "# Evaluation framework\n",
        "from ragas import SingleTurnSample, evaluate\n",
        "from ragas.metrics import (\n",
        "    LLMContextRecall,\n",
        "    Faithfulness,\n",
        "    FactualCorrectness,\n",
        "    ResponseRelevancy,\n",
        "    LLMContextPrecisionWithoutReference,\n",
        "    LLMContextPrecisionWithReference\n",
        ")\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Configure logging\n",
        "logging.getLogger(\"llama_index.llms.openai.utils\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "VVNDPZDiCGai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PDFInfoExtractor:\n",
        "    def __init__(self, input_dir, output_dir, api_key, desc_length=100):\n",
        "        self.input_dir = input_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.desc_length = desc_length\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        if api_key:\n",
        "            generai.configure(api_key=api_key)\n",
        "        elif \"GEMINI_API_KEY\" in os.environ:\n",
        "            generai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "        else:\n",
        "            raise ValueError(\"Please provide gemini_api_key parameter or set GEMINI_API_KEY environment variable\")\n",
        "\n",
        "\n",
        "        # Clear any proxy settings that might interfere\n",
        "        if 'HTTP_PROXY' in os.environ:\n",
        "            del os.environ['HTTP_PROXY']\n",
        "        if 'HTTPS_PROXY' in os.environ:\n",
        "            del os.environ['HTTPS_PROXY']\n",
        "        if 'http_proxy' in os.environ:\n",
        "            del os.environ['http_proxy']\n",
        "        if 'https_proxy' in os.environ:\n",
        "            del os.environ['https_proxy']\n",
        "\n",
        "        # Configure Google Gemini client\n",
        "        self.client = genai.Client(api_key=api_key)\n",
        "\n",
        "    def describe_images_with_llm(self, image_paths):\n",
        "        \"\"\"Send each image to Gemini for structured descriptions.\"\"\"\n",
        "        image_descriptions = []\n",
        "\n",
        "        for image_path in image_paths:\n",
        "            try:\n",
        "                print(f\"Processing image: {image_path}\")\n",
        "\n",
        "                with open(image_path, 'rb') as f:\n",
        "                    image_bytes = f.read()\n",
        "\n",
        "                mime_type, _ = mimetypes.guess_type(image_path)\n",
        "                if not mime_type or not mime_type.startswith('image/'):\n",
        "                    mime_type = 'image/png'\n",
        "\n",
        "                prompt = f\"\"\"Extract and organize all key information from this image for use in a question-answering system. Structure your response as follows:\n",
        "\n",
        "**CONTENT TYPE:** [Identify: chart/graph, diagram, table, text, photo, illustration, etc.]\n",
        "\n",
        "**KEY INFORMATION:**\n",
        "- Main topic or subject matter\n",
        "- Important data points, numbers, or statistics\n",
        "- Key concepts, terms, or labels\n",
        "- Relationships between elements\n",
        "\n",
        "**TEXT CONTENT:**\n",
        "- All visible text (titles, labels, captions, data values)\n",
        "- Any readable content within the image\n",
        "\n",
        "**VISUAL ELEMENTS:**\n",
        "- Charts/graphs: axis labels, data series, trends\n",
        "- Diagrams: components, connections, flow\n",
        "- Tables: headers, key data points\n",
        "- Photos: objects, people, settings, context\n",
        "\n",
        "**CONTEXT & SIGNIFICANCE:**\n",
        "- What this image demonstrates or explains\n",
        "- How it relates to the broader document content\n",
        "\n",
        "Keep response under {self.desc_length} words while maximizing information density for later retrieval.\"\"\"\n",
        "\n",
        "                # Add retry logic with exponential backoff\n",
        "                max_retries = 3\n",
        "                retry_delay = 1\n",
        "\n",
        "                for attempt in range(max_retries):\n",
        "                    try:\n",
        "                        response = self.client.models.generate_content(\n",
        "                            model='gemini-2.5-flash',\n",
        "                            contents=[\n",
        "                                types.Part.from_bytes(\n",
        "                                    data=image_bytes,\n",
        "                                    mime_type=mime_type,\n",
        "                                ),\n",
        "                                prompt\n",
        "                            ]\n",
        "                        )\n",
        "\n",
        "                        description = response.text if response.text else \"No description generated\"\n",
        "                        break\n",
        "\n",
        "                    except Exception as retry_error:\n",
        "                        print(f\"Attempt {attempt + 1} failed: {str(retry_error)}\")\n",
        "                        if attempt < max_retries - 1:\n",
        "                            time.sleep(retry_delay)\n",
        "                            retry_delay *= 2\n",
        "                        else:\n",
        "                            description = f\"Error after {max_retries} attempts: {str(retry_error)}\"\n",
        "\n",
        "                image_descriptions.append({\n",
        "                    \"image_path\": image_path,\n",
        "                    \"description\": description\n",
        "                })\n",
        "\n",
        "                # Add a small delay between API calls to respect rate limits\n",
        "                time.sleep(0.1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error describing image {image_path}: {e}\")\n",
        "                image_descriptions.append({\n",
        "                    \"image_path\": image_path,\n",
        "                    \"description\": f\"Error: Could not generate description - {str(e)}\"\n",
        "                })\n",
        "\n",
        "        return image_descriptions\n",
        "\n",
        "    def extract_metadata(self, pdf_path):\n",
        "        \"\"\"Extracts the most important metadata from a PDF file.\"\"\"\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            metadata = doc.metadata\n",
        "            return {\n",
        "                \"file_name\": os.path.basename(pdf_path),\n",
        "                \"title\": metadata.get(\"title\", \"Unknown\"),\n",
        "                \"author\": metadata.get(\"author\", \"Unknown\"),\n",
        "                \"creation_date\": metadata.get(\"creationDate\", \"Unknown\"),\n",
        "                \"page_count\": len(doc)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_text_and_images(self, pdf_path, extract_images=True):\n",
        "        \"\"\"Extracts text using pdfminer and optionally extracts images using fitz.\"\"\"\n",
        "        extracted_data = {\"text\": \"\", \"images\": []}\n",
        "        image_paths = []\n",
        "\n",
        "        try:\n",
        "            # Text extraction\n",
        "            extracted_data[\"text\"] = extract_text(pdf_path)\n",
        "            print(f\"Extracted text from {pdf_path}\")\n",
        "\n",
        "            if extract_images:\n",
        "                # Images extraction\n",
        "                doc = fitz.open(pdf_path)\n",
        "                for page_num, page in enumerate(doc):\n",
        "                    for img_index, img in enumerate(page.get_images(full=True)):\n",
        "                        xref = img[0]\n",
        "                        base_image = doc.extract_image(xref)\n",
        "                        img_bytes = base_image[\"image\"]\n",
        "                        img_ext = base_image[\"ext\"]\n",
        "\n",
        "                        # Convert to a standard format if needed\n",
        "                        if img_ext not in ['png', 'jpg', 'jpeg', 'webp', 'gif']:\n",
        "                            img_ext = 'png'\n",
        "\n",
        "                        image_path = os.path.join(\n",
        "                            self.output_dir,\n",
        "                            f\"{os.path.basename(pdf_path).replace('.pdf', '')}_page_{page_num+1}_img_{img_index+1}.{img_ext}\"\n",
        "                        )\n",
        "\n",
        "                        # Save the image\n",
        "                        with open(image_path, \"wb\") as img_file:\n",
        "                            img_file.write(img_bytes)\n",
        "\n",
        "                        extracted_data[\"images\"].append({\n",
        "                            \"page\": page_num + 1,\n",
        "                            \"image_path\": image_path,\n",
        "                        })\n",
        "                        image_paths.append(image_path)\n",
        "\n",
        "                # Describing images with Gemini\n",
        "                if image_paths:\n",
        "                    descriptions = self.describe_images_with_llm(image_paths)\n",
        "                    for img in extracted_data[\"images\"]:\n",
        "                        desc = next((d[\"description\"] for d in descriptions if d[\"image_path\"] == img[\"image_path\"]), None)\n",
        "                        if desc:\n",
        "                            img[\"description\"] = desc\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text/images from {pdf_path}: {e}\")\n",
        "\n",
        "        return extracted_data\n",
        "\n",
        "    def process_pdf(self, pdf_file, extract_images=True):\n",
        "        \"\"\"Processes a single PDF file and saves extracted metadata, text, and optionally images.\"\"\"\n",
        "        pdf_path = os.path.join(self.input_dir, pdf_file)\n",
        "        metadata = self.extract_metadata(pdf_path)\n",
        "        extracted_content = self.extract_text_and_images(pdf_path, extract_images)\n",
        "\n",
        "        if metadata:\n",
        "            output_data = {\n",
        "                \"pdf_file\": pdf_file,\n",
        "                \"metadata\": metadata,\n",
        "                \"text\": extracted_content[\"text\"],\n",
        "                \"images\": extracted_content[\"images\"]\n",
        "            }\n",
        "\n",
        "            output_path = os.path.join(self.output_dir, \"extracted_data.json\")\n",
        "\n",
        "            # Load existing data if file exists\n",
        "            if os.path.exists(output_path):\n",
        "                with open(output_path, \"r\", encoding='utf-8') as json_file:\n",
        "                    existing_data = json.load(json_file)\n",
        "            else:\n",
        "                existing_data = []\n",
        "\n",
        "            existing_data.append(output_data)\n",
        "\n",
        "            # Save updated data\n",
        "            with open(output_path, \"w\", encoding='utf-8') as json_file:\n",
        "                json.dump(existing_data, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "            print(f\"Processed: {pdf_file}\")\n",
        "\n",
        "    def process_all_pdfs(self, max_workers=4, extract_images=True):\n",
        "        \"\"\"Processes all PDF files in the input directory in parallel.\"\"\"\n",
        "        pdf_files = [f for f in os.listdir(self.input_dir) if f.endswith(\".pdf\")]\n",
        "\n",
        "        if not pdf_files:\n",
        "            print(\"No PDF files found in the input directory.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Found {len(pdf_files)} PDF files to process...\")\n",
        "\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            executor.map(lambda pdf: self.process_pdf(pdf, extract_images), pdf_files)\n",
        "\n",
        "        print(\"Processing complete!\")"
      ],
      "metadata": {
        "id": "qtt8BXwRCKQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Indexer:\n",
        "    def __init__(self, embedding_model=\"NovaSearch/stella_en_400M_v5\", chunk_size=512, overlap=50, persist_dir=\"./chroma_db\"):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "        self.persist_dir = persist_dir\n",
        "        self.documents = []\n",
        "\n",
        "        self.embed_model = HuggingFaceEmbedding(model_name=self.embedding_model, trust_remote_code=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.db = chromadb.PersistentClient(path=self.persist_dir)\n",
        "        self.index = None\n",
        "\n",
        "    def _check_collection_exists(self, collection_name):\n",
        "        \"\"\"Check if a collection already exists.\"\"\"\n",
        "        try:\n",
        "            self.db.get_collection(collection_name)\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def _get_user_choice_for_existing_collection(self, collection_name):\n",
        "        \"\"\"Ask user what to do with existing collection.\"\"\"\n",
        "        print(f\"\\nCollection '{collection_name}' already exists in the database.\")\n",
        "        print(\"Options:\")\n",
        "        print(\"1. Add new documents to the existing collection\")\n",
        "        print(\"2. Overwrite the collection (delete existing data and create anew)\")\n",
        "        print(\"3. Use the existing collection as-is (no changes)\")\n",
        "\n",
        "        while True:\n",
        "            choice = input(\"Please enter your choice (1/2/3): \").strip()\n",
        "            if choice in ['1', '2', '3']:\n",
        "                return choice\n",
        "            print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "    def _handle_existing_collection(self, collection_name):\n",
        "        \"\"\"Handle existing collection based on user choice.\"\"\"\n",
        "        choice = self._get_user_choice_for_existing_collection(collection_name)\n",
        "\n",
        "        if choice == '1':\n",
        "            print(\"Adding to existing collection...\")\n",
        "            return self.db.get_collection(collection_name), False  # False means don't recreate index\n",
        "        elif choice == '2':\n",
        "            print(\"Overwriting existing collection...\")\n",
        "            self.db.delete_collection(collection_name)\n",
        "            return self.db.create_collection(collection_name), True  # True means recreate index\n",
        "        else:\n",
        "            print(\"Using existing collection as-is...\")\n",
        "            return self.db.get_collection(collection_name), None  # None means use existing without changes\n",
        "\n",
        "    def load_ai_arxiv_data(self):\n",
        "        \"\"\"Load AI research papers from the Hugging Face dataset 'jamescalam/ai-arxiv'.\"\"\"\n",
        "        print(\"Loading dataset from Hugging Face...\")\n",
        "        dataset = datasets.load_dataset(\"jamescalam/ai-arxiv\", split=\"train\")\n",
        "        return dataset\n",
        "\n",
        "    def setup_vector_store_from_arxiv(self, dataset_name=\"ai_arxiv\"):\n",
        "        \"\"\"Load, preprocess, and create a vector store from the AI ArXiv dataset.\"\"\"\n",
        "        start_time = time.time()\n",
        "        dataset = self.load_ai_arxiv_data()\n",
        "\n",
        "        all_docs = []\n",
        "        for paper in tqdm(dataset, desc=\"Processing papers\"):\n",
        "            metadata = {\n",
        "                \"title\": paper[\"title\"],\n",
        "                \"primary_category\": paper[\"primary_category\"],\n",
        "                \"published\": paper[\"published\"],\n",
        "                \"source\": paper[\"source\"]\n",
        "            }\n",
        "\n",
        "            all_docs.extend([{\"content\": paper[\"content\"], \"metadata\": metadata}])\n",
        "\n",
        "        self.documents = [Document(text=chunk[\"content\"], metadata=chunk[\"metadata\"]) for chunk in all_docs]\n",
        "        self.collection_name = f\"collection_{dataset_name}_{self.chunk_size}\"\n",
        "\n",
        "        if self._check_collection_exists(self.collection_name):\n",
        "            chroma_collection, should_recreate_index = self._handle_existing_collection(self.collection_name)\n",
        "        else:\n",
        "            print(f\"Creating new collection: {self.collection_name}\")\n",
        "            chroma_collection = self.db.create_collection(self.collection_name)\n",
        "            should_recreate_index = True\n",
        "\n",
        "        splitter = SentenceSplitter(chunk_size=self.chunk_size, chunk_overlap=self.overlap)\n",
        "\n",
        "        if should_recreate_index is True:\n",
        "            # Create new index\n",
        "            self.index = VectorStoreIndex.from_documents(\n",
        "                self.documents,\n",
        "                storage_context=StorageContext.from_defaults(\n",
        "                    vector_store=ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "                ),\n",
        "                embed_model=self.embed_model,\n",
        "                transformations=[splitter]\n",
        "            )\n",
        "        elif should_recreate_index is False:\n",
        "            # Add to existing index\n",
        "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "            # Load existing index\n",
        "            self.index = VectorStoreIndex.from_vector_store(\n",
        "                vector_store=vector_store,\n",
        "                embed_model=self.embed_model\n",
        "            )\n",
        "\n",
        "            # Add new documents to existing index\n",
        "            for document in self.documents:\n",
        "                self.index.insert(document, transformations=[splitter])\n",
        "        else:\n",
        "            # Load existing index without adding new documents\n",
        "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "            self.index = VectorStoreIndex.from_vector_store(\n",
        "                vector_store=vector_store,\n",
        "                embed_model=self.embed_model\n",
        "            )\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Vector store setup completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "        db = chromadb.PersistentClient(path=self.persist_dir)\n",
        "        collection = db.get_collection(self.collection_name)\n",
        "        result = collection.get()\n",
        "        chunks = list(zip(result['ids'], result['documents']))\n",
        "        print(f\"Total num of chunks: {len(chunks)}\")\n",
        "\n",
        "        return self.index\n",
        "\n",
        "    def load_extracted_data(self, file_path):\n",
        "        \"\"\"Load and preprocess extracted data from a single JSON file.\"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            extracted_data = json.load(file)\n",
        "\n",
        "        all_chunks = []\n",
        "        for pdf_data in extracted_data:\n",
        "            text_chunks = self.chunk_text(pdf_data.get(\"text\", \"\"), pdf_data.get(\"metadata\", {}))\n",
        "            image_chunks = self.chunk_image_descriptions(pdf_data.get(\"images\", []), pdf_data.get(\"metadata\", {}))\n",
        "            all_chunks.extend(text_chunks + image_chunks)\n",
        "\n",
        "        return all_chunks\n",
        "\n",
        "    def chunk_text(self, text, metadata):\n",
        "        \"\"\"Split text into manageable chunks using SentenceSplitter.\"\"\"\n",
        "        splitter = SentenceSplitter(chunk_size=self.chunk_size, chunk_overlap=self.overlap)\n",
        "        text_chunks = splitter.split_text(text)\n",
        "        return [{\"content\": chunk, \"type\": \"text\", \"metadata\": metadata} for chunk in text_chunks]\n",
        "\n",
        "    def chunk_image_descriptions(self, image_descriptions, metadata):\n",
        "        \"\"\"Extract meaningful text from image descriptions (no splitting).\"\"\"\n",
        "        chunks = []\n",
        "        for img in image_descriptions:\n",
        "            img_text = f\"Image from page {img['page']}: {img['description']}\"\n",
        "            chunks.append({\"content\": img_text, \"type\": \"image\", \"metadata\": metadata})\n",
        "        return chunks\n",
        "\n",
        "    def setup_vector_store(self, dataset_name=\"pdf-dataset\", file_path=\"/content/data/extracted/extracted_data.json\"):\n",
        "        \"\"\"Loads data, creates embeddings, and stores in ChromaDB with metadata.\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        all_chunks = self.load_extracted_data(file_path)\n",
        "        self.documents = [\n",
        "            Document(text=chunk[\"content\"], metadata={**chunk[\"metadata\"], \"type\": chunk[\"type\"]})\n",
        "            for chunk in all_chunks\n",
        "        ]\n",
        "        self.collection_name = f\"collection_{dataset_name}_{self.chunk_size}\"\n",
        "\n",
        "        if self._check_collection_exists(self.collection_name):\n",
        "            chroma_collection, should_recreate_index = self._handle_existing_collection(self.collection_name)\n",
        "        else:\n",
        "            print(f\"Creating new collection: {self.collection_name}\")\n",
        "            chroma_collection = self.db.create_collection(self.collection_name)\n",
        "            should_recreate_index = True\n",
        "\n",
        "        if should_recreate_index is True:\n",
        "            # Create new index\n",
        "            self.index = VectorStoreIndex.from_documents(\n",
        "                self.documents,\n",
        "                storage_context=StorageContext.from_defaults(\n",
        "                    vector_store=ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "                ),\n",
        "                embed_model=self.embed_model\n",
        "            )\n",
        "        elif should_recreate_index is False:\n",
        "            # Add to existing index\n",
        "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "            # Load existing index\n",
        "            self.index = VectorStoreIndex.from_vector_store(\n",
        "                vector_store=vector_store,\n",
        "                embed_model=self.embed_model\n",
        "            )\n",
        "\n",
        "            # Add new documents to existing index\n",
        "            for document in self.documents:\n",
        "                self.index.insert(document)\n",
        "        else:\n",
        "            # Load existing index without adding new documents\n",
        "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "            self.index = VectorStoreIndex.from_vector_store(\n",
        "                vector_store=vector_store,\n",
        "                embed_model=self.embed_model\n",
        "            )\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Vector store setup completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "        return self.index"
      ],
      "metadata": {
        "id": "fPSeKYqvCMNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreRetrieval:\n",
        "    def __init__(self, model=\"gemini-2.5-flash\", gemini_api_key=None):\n",
        "        if gemini_api_key:\n",
        "            generai.configure(api_key=gemini_api_key)\n",
        "        elif \"GEMINI_API_KEY\" in os.environ:\n",
        "            generai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "        else:\n",
        "            raise ValueError(\"Please provide gemini_api_key parameter or set GEMINI_API_KEY environment variable\")\n",
        "\n",
        "\n",
        "        self.model_name = model\n",
        "        self.model = generai.GenerativeModel(model)\n",
        "\n",
        "        self.generation_config = generai.types.GenerationConfig(\n",
        "            temperature=0.0,\n",
        "            candidate_count=1,\n",
        "            max_output_tokens=None,\n",
        "        )\n",
        "\n",
        "    def rewrite_query(self, original_query):\n",
        "        \"\"\"Rewrites original query to be more specific and retrieval-friendly.\"\"\"\n",
        "        prompt = f\"\"\"You are an expert at reformulating search queries for better information retrieval. Your task is to rewrite the user's query to be more specific, detailed, and likely to match relevant documents.\n",
        "\n",
        "Guidelines for rewriting:\n",
        "- Add technical terminology and synonyms\n",
        "- Specify the context or domain when unclear\n",
        "- Expand abbreviations and acronyms including both forms\n",
        "- Make implicit concepts and relationships explicit\n",
        "- Include alternative phrasings that experts might use\n",
        "- Add specific keywords that would appear in authoritative sources\n",
        "- Maintain the original intent\n",
        "\n",
        "Examples:\n",
        "Original: \"What is hybrid retrieval?\"\n",
        "Rewritten: \"What are hybrid retrieval methods in information retrieval systems that combine dense vector search and sparse keyword matching techniques?\"\n",
        "\n",
        "Original: \"How does attention work?\"\n",
        "Rewritten: \"How do attention mechanisms work in neural networks, including self-attention and cross-attention for sequence processing and transformer architectures?\"\n",
        "\n",
        "Original: \"RAG performance issues\"\n",
        "Rewritten: \"What are common performance bottlenecks and optimization strategies for Retrieval-Augmented Generation systems including chunking, embedding quality, and retrieval accuracy?\"\n",
        "\n",
        "Now rewrite this query:\n",
        "Original: \"{original_query}\"\n",
        "Rewritten:\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=self.generation_config\n",
        "            )\n",
        "\n",
        "            rewritten = response.text.strip()\n",
        "\n",
        "            # If the response contains \"Rewritten:\" try to extract after it\n",
        "            if \"rewritten:\" in rewritten.lower():\n",
        "                parts = rewritten.split(\":\")\n",
        "                if len(parts) > 1:\n",
        "                    rewritten = parts[-1].strip()\n",
        "\n",
        "            rewritten = rewritten.strip('\"\\'`')\n",
        "            return rewritten if rewritten else original_query\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in rewrite_query: {e}\")\n",
        "            return original_query\n",
        "\n",
        "        def decompose_query(self, original_query: str, direct_answer: str = \"N/A\", max_sub_queries: int = 3) -> list:\n",
        "          \"\"\"Decomposes a complex query into focused sub-questions.\"\"\"\n",
        "          prompt = f\"\"\"You are decomposing a complex query into simpler, independent sub-questions for question answering.\n",
        "\n",
        "Original Query: {original_query}\n",
        "Direct Answer Attempt: {direct_answer}\n",
        "\n",
        "Break down the original query into {max_sub_queries} or fewer specific, independent sub-questions that:\n",
        "1. Address different aspects or components of the original query\n",
        "2. Can be answered independently (each sub-question should make sense on its own)\n",
        "3. Are more specific and focused than the original query\n",
        "4. Don't overlap significantly with each other\n",
        "5. Together will provide information needed to comprehensively answer the original query\n",
        "6. If a direct answer was provided, target any gaps or missing information\n",
        "\n",
        "Example:\n",
        "Complex query: \"How do transformer models handle long sequences and what are the computational challenges?\"\n",
        "\n",
        "Sub-questions:\n",
        "1. What is the computational complexity of attention mechanisms in transformer models?\n",
        "2. What specific techniques exist for handling long sequences in transformers like sparse attention and sliding window?\n",
        "3. How do memory requirements scale with sequence length in transformer architectures?\n",
        "\n",
        "Now decompose this query:\n",
        "Complex query: \"{original_query}\"\n",
        "\n",
        "Sub-questions:\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=self.generation_config\n",
        "            )\n",
        "\n",
        "            raw_output = response.text\n",
        "\n",
        "            sub_questions = []\n",
        "            lines = raw_output.split('\\n')\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                # Match various numbering formats: \"1.\", \"1)\", \"1 -\", etc.\n",
        "                match = re.match(r'^\\d+[\\.\\)\\-\\s]+(.+)', line)\n",
        "                if match:\n",
        "                    question = match.group(1).strip()\n",
        "                    if question:  # Only add non-empty questions\n",
        "                        sub_questions.append(question)\n",
        "\n",
        "            return sub_questions if sub_questions else [original_query]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in decompose_query: {e}\")\n",
        "            return [original_query]\n",
        "\n",
        "\n",
        "    def hyde(self, query):\n",
        "        \"\"\"Generates a hypothetical document that would answer the query.\"\"\"\n",
        "        prompt = f\"\"\"You are an expert technical writer. Generate a well-structured, informative document that would perfectly answer the given query. This document will be used to improve retrieval by finding similar real documents.\n",
        "\n",
        "Guidelines:\n",
        "- Write in an authoritative, encyclopedia-style tone\n",
        "- Include specific technical details, terminology, and concepts\n",
        "- Structure with clear sections if appropriate\n",
        "- Aim for 200-400 words\n",
        "- Focus on factual, detailed information\n",
        "- Use domain-specific vocabulary that would appear in relevant documents\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "Document:\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=self.generation_config\n",
        "            )\n",
        "\n",
        "            generated_doc = response.text.strip()\n",
        "\n",
        "            # Clean up the response if it contains the word \"Document:\"\n",
        "            if \"document:\" in generated_doc.lower():\n",
        "                parts = generated_doc.split(\":\")\n",
        "                if len(parts) > 1:\n",
        "                    generated_doc = \":\".join(parts[1:]).strip()\n",
        "\n",
        "            return generated_doc\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in hyde: {e}\")\n",
        "            return f\"A comprehensive guide to {query}, covering key concepts, methodologies, and practical applications in the field.\""
      ],
      "metadata": {
        "id": "Ci7_r62cCOGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Retriever:\n",
        "    def __init__(self, indexing_module, top_k=5):\n",
        "        self.index = indexing_module.index\n",
        "        self.top_k = top_k\n",
        "        documents = indexing_module.documents\n",
        "\n",
        "        self.bm25_retriever = BM25Retriever.from_defaults(\n",
        "            nodes=self._create_nodes_from_documents(documents),\n",
        "            similarity_top_k=top_k\n",
        "        )\n",
        "        self.vectorstore_retriever = self.index.as_retriever(similarity_top_k=top_k)\n",
        "\n",
        "    def dense_retrieve(self, query):\n",
        "        \"\"\"Performs dense retrieval using vector similarity.\"\"\"\n",
        "        return self.vectorstore_retriever.retrieve(query)\n",
        "\n",
        "    def sparse_retrieve(self, query):\n",
        "        \"\"\"Performs sparse retrieval using BM25.\"\"\"\n",
        "        return self.bm25_retriever.retrieve(query)\n",
        "\n",
        "    def _create_nodes_from_documents(self, documents):\n",
        "        \"\"\"\n",
        "        Convert documents to nodes for BM25Retriever.\n",
        "        \"\"\"\n",
        "        parser = SimpleNodeParser.from_defaults()\n",
        "        all_nodes = []\n",
        "\n",
        "        # Parse each document into nodes\n",
        "        for doc in documents:\n",
        "            nodes = parser.get_nodes_from_documents([doc])\n",
        "            all_nodes.extend(nodes)\n",
        "\n",
        "        return all_nodes\n",
        "\n",
        "    def hybrid_retrieve(self, query, alpha=0.7, mode = FUSION_MODES.RECIPROCAL_RANK):\n",
        "        \"\"\"Combines BM25 results with vector search\"\"\"\n",
        "        hybrid_retriever = QueryFusionRetriever(\n",
        "            retrievers=[\n",
        "                self.vectorstore_retriever,\n",
        "                self.bm25_retriever\n",
        "            ],\n",
        "            mode = mode,\n",
        "            llm=MockLLM(),\n",
        "            num_queries=1,\n",
        "            use_async=False,\n",
        "            retriever_weights=[alpha, 1-alpha],\n",
        "            similarity_top_k=self.top_k\n",
        "        )\n",
        "\n",
        "        return hybrid_retriever.retrieve(query)\n",
        "\n",
        "\n",
        "    def duckduckgo_search(self, query, top_k=5):\n",
        "        \"\"\"\n",
        "        Performs web search using DuckDuckGo and returns results as Document objects.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            ddgs = DDGS()\n",
        "            search_results = list(ddgs.text(query, max_results=top_k))\n",
        "\n",
        "            documents = []\n",
        "            for i, result in enumerate(search_results):\n",
        "                if 'body' in result and 'href' in result and 'title' in result:\n",
        "                    doc_text = f\"Title: {result['title']}\\nURL: {result['href']}\\n\\n{result['body']}\"\n",
        "                    doc = Document(\n",
        "                        text=doc_text,\n",
        "                        metadata={\n",
        "                            \"source\": result['href'],\n",
        "                            \"title\": result['title'],\n",
        "                            \"rank\": i + 1\n",
        "                        }\n",
        "                    )\n",
        "                    documents.append(doc)\n",
        "\n",
        "            return documents\n",
        "        except Exception as e:\n",
        "            logging.error(f\"DuckDuckGo search error: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def wikipedia_search(self, query, sentences=5):\n",
        "        \"\"\"\n",
        "        Searches Wikipedia for the query and returns results as Document objects.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Search for Wikipedia articles\n",
        "            search_results = wikipedia.search(query)\n",
        "            documents = []\n",
        "\n",
        "            for title in search_results[:5]:  # Limit to top 5 results\n",
        "                try:\n",
        "                    # Get the page content\n",
        "                    page = wikipedia.page(title, auto_suggest=False)\n",
        "\n",
        "                    # Get a summary of the page\n",
        "                    summary = wikipedia.summary(title, sentences=sentences, auto_suggest=False)\n",
        "\n",
        "                    doc = Document(\n",
        "                        text=f\"Title: {title}\\nURL: {page.url}\\n\\nSummary: {summary}\",\n",
        "                        metadata={\n",
        "                            \"source\": page.url,\n",
        "                            \"title\": title,\n",
        "                            \"type\": \"wikipedia\"\n",
        "                        }\n",
        "                    )\n",
        "                    documents.append(doc)\n",
        "                except (wikipedia.exceptions.DisambiguationError,\n",
        "                        wikipedia.exceptions.PageError) as e:\n",
        "                    logging.warning(f\"Wikipedia error for '{title}': {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            return documents\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Wikipedia search error: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def web_search_retrieve(self, query, use_duckduckgo=True, use_wikipedia=True, top_k=5):\n",
        "        \"\"\"\n",
        "        Performs a combined web search using multiple sources.\n",
        "        \"\"\"\n",
        "        documents = []\n",
        "\n",
        "        if use_duckduckgo:\n",
        "            ddg_docs = self.duckduckgo_search(query, top_k)\n",
        "            documents.extend(ddg_docs)\n",
        "\n",
        "        if use_wikipedia:\n",
        "            wiki_docs = self.wikipedia_search(query)\n",
        "            documents.extend(wiki_docs)\n",
        "\n",
        "        return documents"
      ],
      "metadata": {
        "id": "yAtYwegICQQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PostRetrieval:\n",
        "    def __init__(self, model=\"BAAI/bge-reranker-base\", num_to_return = 5):\n",
        "        self.model = model\n",
        "        self.num_to_return = num_to_return\n",
        "\n",
        "    def rerank(self, query, retrieved_docs):\n",
        "        seen_texts = set()\n",
        "        unique_docs = []\n",
        "        for doc in retrieved_docs:\n",
        "            if doc.text not in seen_texts:\n",
        "                seen_texts.add(doc.text)\n",
        "                unique_docs.append(doc)\n",
        "\n",
        "        self.reranker = CrossEncoder(self.model, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        pairs = [(query, doc.text) for doc in retrieved_docs]\n",
        "        scores = self.reranker.predict(pairs)\n",
        "\n",
        "        doc_scores = {id(doc): score for doc, score in zip(retrieved_docs, scores)}\n",
        "        ranked_docs = sorted(retrieved_docs, key=lambda x: doc_scores[id(x)], reverse=True)\n",
        "\n",
        "        return ranked_docs[:self.num_to_return]"
      ],
      "metadata": {
        "id": "3LQ2aCPyCRvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator:\n",
        "    def __init__(self, model=\"gemini-2.5-flash\", gemini_api_key=None):\n",
        "        if gemini_api_key:\n",
        "            generai.configure(api_key=gemini_api_key)\n",
        "        elif \"GEMINI_API_KEY\" in os.environ:\n",
        "            generai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "        else:\n",
        "            raise ValueError(\"Please provide gemini_api_key parameter or set GEMINI_API_KEY environment variable\")\n",
        "\n",
        "\n",
        "        self.model_name = model\n",
        "        self.model = generai.GenerativeModel(model)\n",
        "\n",
        "        self.generation_config = generai.types.GenerationConfig(\n",
        "            temperature=0.0,\n",
        "            candidate_count=1,\n",
        "            max_output_tokens=None,\n",
        "        )\n",
        "\n",
        "    def generate(self, query, documents):\n",
        "        \"\"\"Generates a response using Gemini based on retrieved documents.\"\"\"\n",
        "        context = \"\\n\\n\".join([doc.text for doc in documents]) if documents else None\n",
        "\n",
        "        if context:\n",
        "            prompt = f\"\"\"You are a helpful AI assistant tasked with answering questions accurately and clearly.\n",
        "\n",
        "Guidelines:\n",
        "- Use the information provided in the Context to answer the Query.\n",
        "- If the Context does NOT contain sufficient or relevant information to answer the Query, explicitly state what information cannot be found in the given Context.\n",
        "- Provide a clear, concise, and complete response based strictly on the Context.\n",
        "- If there is any uncertainty or ambiguity in the Context regarding the Query, acknowledge this uncertainty clearly.\n",
        "- Avoid adding any information not present in the Context.\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"You are a helpful AI assistant tasked with answering questions accurately and clearly.\n",
        "\n",
        "Guidelines:\n",
        "- Answer the Query to the best of your knowledge.\n",
        "- Provide a clear, concise, and complete response based on your own knowledge.\n",
        "- If you are unsure about any detail, indicate that clearly.\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        response = self.model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=self.generation_config\n",
        "        )\n",
        "\n",
        "        return response.text"
      ],
      "metadata": {
        "id": "42CNjmv4CUSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OrchestrationModule:\n",
        "    def __init__(self, model=\"gemini-2.5-flash\", gemini_api_key=None):\n",
        "        if gemini_api_key:\n",
        "            generai.configure(api_key=gemini_api_key)\n",
        "        elif \"GEMINI_API_KEY\" in os.environ:\n",
        "            generai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "        else:\n",
        "            raise ValueError(\"Please provide gemini_api_key parameter or set GEMINI_API_KEY environment variable\")\n",
        "\n",
        "\n",
        "        self.model_name = model\n",
        "        self.model = generai.GenerativeModel(model)\n",
        "\n",
        "        self.generation_config = generai.types.GenerationConfig(\n",
        "            temperature=0.0,\n",
        "            candidate_count=1,\n",
        "            max_output_tokens=None,\n",
        "        )\n",
        "\n",
        "    def _generate_text(self, prompt: str) -> str:\n",
        "        response = self.model.generate_content(prompt, generation_config=self.generation_config)\n",
        "        return response.text.strip()\n",
        "\n",
        "    def judge_retrieval_necessity(self, query):\n",
        "        prompt = f\"\"\"\n",
        "You are an AI system that determines whether external retrieval is necessary.\n",
        "If the query can be answered without retrieval, return 'NO'.\n",
        "If the query requires additional information, return 'YES'.\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Retrieval Needed:\n",
        "\"\"\"\n",
        "        result = self._generate_text(prompt)\n",
        "        print(f\"Retrieval necessity judgment: {result}\")\n",
        "        return \"YES\" in result.upper()\n",
        "\n",
        "    def judge_query_needs_web_search(self, query):\n",
        "        prompt = f\"\"\"\n",
        "You are an AI system that determines whether a web search is necessary to properly answer a query.\n",
        "Analyze the query to determine if it requires recent, real-time, or web information that might not be in a static knowledge base.\n",
        "\n",
        "Consider these factors:\n",
        "1. Does the query ask about recent events, news, or current information?\n",
        "2. Does the query reference time-sensitive data (prices, weather, scores, etc.)?\n",
        "3. Does the query ask about trending topics or recent developments?\n",
        "4. Would the answer to this query likely change frequently or have been updated since the knowledge base was last updated?\n",
        "5. Is the query about a very specific or niche topic that might not be well-covered in a general knowledge base?\n",
        "\n",
        "If the query can be answered with timeless information from a knowledge base, return 'NO'.\n",
        "If the query would benefit from recent web information, return 'YES'.\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Web Search Needed:\n",
        "\"\"\"\n",
        "        result = self._generate_text(prompt)\n",
        "        print(f\"Web search judgment: {result}\")\n",
        "        return \"YES\" in result.upper()\n",
        "\n",
        "    def judge_iteration_completeness(self, original_query, generated_text):\n",
        "        prompt = f\"\"\"\n",
        "You are evaluating whether a generated answer has sufficient information to fully answer a query.\n",
        "\n",
        "Original Query: {original_query}\n",
        "Generated Answer So Far: {generated_text}\n",
        "\n",
        "Consider:\n",
        "1. Does the generated answer comprehensively and fully address the query?\n",
        "2. Are there any gaps in the information needed to answer the query?\n",
        "3. Would additional search likely provide more relevant information?\n",
        "4. Are there any aspects of the query that remain unaddressed?\n",
        "\n",
        "If the generated answer fully answers the original query, return 'COMPLETE'.\n",
        "If more search/refinement would be beneficial, return 'CONTINUE'.\n",
        "\n",
        "Respond with ONLY one word: 'COMPLETE' or 'CONTINUE'\n",
        "Judgement:\n",
        "\"\"\"\n",
        "        result = self._generate_text(prompt)\n",
        "        print(f\"Iteration completeness judgment: {result}\")\n",
        "        return \"COMPLETE\" in result.upper()\n",
        "\n",
        "    def judge_query_refinement_completeness(self, original_query, current_query, iteration, current_answer):\n",
        "        prompt = f\"\"\"\n",
        "You are evaluating whether recursive query refinement should continue.\n",
        "\n",
        "Original Query: {original_query}\n",
        "Current Refined Query: {current_query}\n",
        "Current Answer: {current_answer}\n",
        "Iteration Number: {iteration}\n",
        "\n",
        "Consider:\n",
        "1. Does the current answer adequately address the original query?\n",
        "2. Are there significant gaps in information that could be filled with better query refinement?\n",
        "3. Would further query refinement likely improve result quality significantly?\n",
        "4. Are we getting diminishing returns from further refinement?\n",
        "\n",
        "If the current query and answer are sufficient, return 'COMPLETE'.\n",
        "If further refinement would be beneficial, return 'CONTINUE'.\n",
        "\n",
        "Judgment:\n",
        "\"\"\"\n",
        "        result = self._generate_text(prompt)\n",
        "        print(f\"Query refinement completeness judgment: {result}\")\n",
        "\n",
        "        judgment_match = re.search(r'(?:Final\\s+)?Judgment:\\s*(\\w+)', result, re.IGNORECASE)\n",
        "        if judgment_match:\n",
        "            final_judgment = judgment_match.group(1).upper()\n",
        "            return final_judgment == \"COMPLETE\"\n",
        "\n",
        "        # Fallback: check last line\n",
        "        lines = result.strip().split('\\n')\n",
        "        last_line = lines[-1].upper()\n",
        "        return \"COMPLETE\" in last_line and \"CONTINUE\" not in last_line\n",
        "\n",
        "\n",
        "    def refine_query_recursively(self, original_query, current_query, current_answer, iteration):\n",
        "        prompt = f\"\"\"\n",
        "You are refining a search query recursively to improve search results and fill information gaps.\n",
        "\n",
        "Original Query: {original_query}\n",
        "Current Query: {current_query}\n",
        "Current Answer: {current_answer}\n",
        "Iteration: {iteration}\n",
        "\n",
        "Analyze the current answer and identify what information is still missing or insufficient to fully answer the original query.\n",
        "\n",
        "Refine the query to:\n",
        "1. Focus on the missing information gaps identified in the current answer\n",
        "2. Target specific aspects that are incomplete or unclear\n",
        "3. Avoid repeating information already well-covered in current results\n",
        "4. Maintain alignment with the original query's intent\n",
        "5. Be more specific about the missing pieces needed\n",
        "\n",
        "Create a refined query that will retrieve information to fill the gaps in the current answer.\n",
        "\n",
        "Refined Query:\n",
        "\"\"\"\n",
        "        result = self._generate_text(prompt)\n",
        "\n",
        "        if \"Refined Query:\" in result:\n",
        "            refined_query = result.split(\"Refined Query:\")[-1].strip()\n",
        "        else:\n",
        "            # Fallback: take the last line or use the full result\n",
        "            refined_query = result.strip().split('\\n')[-1].strip()\n",
        "\n",
        "        refined_query = refined_query.strip('\"').strip(\"'\").strip()\n",
        "\n",
        "        return refined_query\n",
        "\n",
        "    def synthesize_recursive_answer(self, original_query: str, direct_answer: str, sub_results: list) -> str:\n",
        "        \"\"\"Synthesize final answer from sub-results and contexts in recursive RAG\"\"\"\n",
        "        # Prepare sub-results summary\n",
        "        sub_answers_text = \"\"\n",
        "        for i, result in enumerate(sub_results, 1):\n",
        "            sub_answers_text += f\"\\nSub-question {i}: {result['query']}\\nAnswer: {result['answer']}\\n\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You need to synthesize a focused, concise answer by combining only the most relevant information.\n",
        "\n",
        "Original Query: {original_query}\n",
        "Direct Answer (may be incomplete): {direct_answer}\n",
        "\n",
        "Sub-questions and their answers:\n",
        "{sub_answers_text}\n",
        "\n",
        "Instructions:\n",
        "1. Extract ONLY information directly relevant to answering the original query\n",
        "2. Prioritize the most important points that address the core question\n",
        "3. Integrate information from direct answer and sub-answers into a coherent response\n",
        "4. Be concise - include only what's necessary to fully answer the query\n",
        "5. Avoid unnecessary details or tangential information\n",
        "6. Maintain natural flow without repetition or redundancy\n",
        "7. Focus on providing a complete but succinct answer\n",
        "\n",
        "Synthesized Answer:\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            result = self._generate_text(prompt)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"Error synthesizing answer: {e}\")\n",
        "            # Fallback: return direct answer with sub-answers appended\n",
        "            fallback = f\"{direct_answer}\\n\\nAdditional information:\\n\"\n",
        "            for result in sub_results:\n",
        "                fallback += f\"- {result['answer']}\\n\"\n",
        "            return fallback\n"
      ],
      "metadata": {
        "id": "4Q4TGBfJCWCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, llm_model=\"gemini-2.0-flash\", api_key=None):\n",
        "        \"\"\"Initialize evaluator with a Gemini model.\"\"\"\n",
        "        if api_key is None:\n",
        "            raise ValueError(\n",
        "                \"Please provide api_key parameter, store it in Colab userdata, \"\n",
        "                \"or set the GOOGLE_API_KEY environment variable.\"\n",
        "            )\n",
        "\n",
        "        gemini_llm = ChatGoogleGenerativeAI(\n",
        "            model=llm_model,\n",
        "            temperature=0.0,\n",
        "            max_tokens=None,\n",
        "            timeout=None,\n",
        "            max_retries=2,\n",
        "            google_api_key=api_key\n",
        "        )\n",
        "\n",
        "        self.llm = LangchainLLMWrapper(gemini_llm)\n",
        "\n",
        "        self.context_precision = LLMContextPrecisionWithoutReference(llm=self.llm)\n",
        "        self.context_precision_with_ref = LLMContextPrecisionWithReference(llm=self.llm)\n",
        "\n",
        "    def check_answer_quality(self, query, answer, ground_truth, contexts=None, model=\"gemini-2.0-flash\"):\n",
        "        \"\"\"\n",
        "        Evaluate the quality of an answer against its ground truth using RAGAS metrics.\n",
        "        \"\"\"\n",
        "        # Create a base dataset dictionary\n",
        "        data_sample = {\n",
        "            'question': [query],\n",
        "            'answer': [answer],\n",
        "            'ground_truth': ground_truth,\n",
        "            'retrieved_contexts': [contexts]\n",
        "        }\n",
        "        dataset = Dataset.from_dict(data_sample)\n",
        "\n",
        "        if contexts is None:\n",
        "            metrics = [FactualCorrectness(mode=\"recall\")]\n",
        "        else:\n",
        "            metrics = [FactualCorrectness(mode=\"recall\"), LLMContextRecall(), Faithfulness()]\n",
        "\n",
        "        scores = evaluate(dataset, metrics=metrics, llm=self.llm)\n",
        "\n",
        "        dict_scores = scores.to_pandas().iloc[0].to_dict()\n",
        "        results = {}\n",
        "        for metric in dict_scores.keys():\n",
        "            results[metric] = dict_scores[metric]\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def evaluate_context_precision_without_ref(self, retrieved_contexts, query, response):\n",
        "        \"\"\"Evaluate context precision for a query without reference.\"\"\"\n",
        "        sample = SingleTurnSample(\n",
        "            user_input=query,\n",
        "            response=response,\n",
        "            retrieved_contexts=retrieved_contexts,\n",
        "        )\n",
        "        score = await self.context_precision.single_turn_ascore(sample)\n",
        "        print(f\"context precision without reference: {score}\")\n",
        "        return score\n",
        "\n",
        "    async def evaluate_context_precision_with_ref(self, retrieved_contexts, query, reference):\n",
        "        \"\"\"Evaluate context precision for a query with reference.\"\"\"\n",
        "        sample = SingleTurnSample(\n",
        "            user_input=query,\n",
        "            reference=reference,\n",
        "            retrieved_contexts=retrieved_contexts,\n",
        "        )\n",
        "        score = await self.context_precision_with_ref.single_turn_ascore(sample)\n",
        "        print(f\"context precision with reference: {score}\\n\")\n",
        "        return score\n",
        "\n",
        "    def is_failed_metric(self, value):\n",
        "        \"\"\"Check if a metric value indicates failure (error, NaN, None, etc.)\"\"\"\n",
        "        if value is None:\n",
        "            return True\n",
        "        if isinstance(value, dict) and \"error\" in value:\n",
        "            return True\n",
        "        if isinstance(value, float) and (math.isnan(value) or math.isinf(value)):\n",
        "            return True\n",
        "        if isinstance(value, str) and value.lower() in [\"nan\", \"error\", \"none\"]:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def needs_retry(self, result):\n",
        "        \"\"\"Check if a result needs retry due to failed metrics\"\"\"\n",
        "        if \"error\" in result:\n",
        "            return True\n",
        "\n",
        "        # Check main metrics\n",
        "        if \"metrics\" in result:\n",
        "            for metric_name, value in result[\"metrics\"].items():\n",
        "                if self.is_failed_metric(value):\n",
        "                    return True\n",
        "\n",
        "        # Check context precision\n",
        "        if \"context_precision_with_ref\" in result:\n",
        "            if self.is_failed_metric(result[\"context_precision_with_ref\"]):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def evaluate_single_qa(self, rag_system, qa, qa_index, existing_result=None):\n",
        "        \"\"\"Evaluate a single QA pair, optionally retrying failed metrics from existing result\"\"\"\n",
        "        print(f\"\\n--- Question {qa_index+1} ---\")\n",
        "        print(f\"Q: {qa['query']}\")\n",
        "\n",
        "        # Use existing answer and contexts if available and not retrying everything\n",
        "        if existing_result and not existing_result.get(\"error\"):\n",
        "            answer = existing_result.get(\"answer\")\n",
        "            contexts = existing_result.get(\"contexts\", [])\n",
        "            print(f\"Generated Answer: {answer} (reusing)\")\n",
        "            print(f\"Contexts: {len(contexts)} (reusing)\")\n",
        "        else:\n",
        "            try:\n",
        "                answer, contexts = rag_system.process_query(qa['query'])\n",
        "                contexts = [c.text if hasattr(c, \"text\") else str(c) for c in contexts] if contexts else []\n",
        "                print(f\"Generated Answer: {answer}\")\n",
        "                print(f\"Contexts: {len(contexts)}\")\n",
        "                print(f\"Ground truth: {qa['generation_gt'][0]}\")\n",
        "            except Exception as e:\n",
        "                return {\n",
        "                    \"id\": qa_index,\n",
        "                    \"question\": qa['query'],\n",
        "                    \"error\": f\"RAG system error: {str(e)}\",\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                }\n",
        "\n",
        "        result = {\n",
        "            \"id\": qa_index,\n",
        "            \"question\": qa['query'],\n",
        "            \"answer\": answer,\n",
        "            \"ground_truth\": qa['generation_gt'][0],\n",
        "            \"contexts\": contexts,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Evaluate answer quality (retry if previous failed or missing)\n",
        "        eval_results = {}\n",
        "        if (existing_result and \"metrics\" in existing_result and\n",
        "            not any(self.is_failed_metric(v) for v in existing_result[\"metrics\"].values())):\n",
        "            eval_results = existing_result[\"metrics\"]\n",
        "            print(\"  Reusing existing answer quality metrics\")\n",
        "        else:\n",
        "            try:\n",
        "                eval_results = self.check_answer_quality(\n",
        "                    qa['query'], answer, qa['generation_gt'], contexts\n",
        "                )\n",
        "                print(\"  Evaluated answer quality metrics:\")\n",
        "            except Exception as e:\n",
        "                eval_results = {\"error\": f\"Answer quality evaluation error: {str(e)}\"}\n",
        "                print(f\"  Answer quality evaluation failed: {e}\")\n",
        "\n",
        "        if isinstance(eval_results, dict) and \"error\" not in eval_results:\n",
        "            for metric, value in eval_results.items():\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"    {metric}: {value:.3f}\")\n",
        "\n",
        "        result[\"metrics\"] = eval_results\n",
        "\n",
        "        time.sleep(15) # To prevent rate limits errors\n",
        "\n",
        "        # Evaluate context precision (retry if previous failed or missing)\n",
        "        context_score = None\n",
        "        if (existing_result and \"context_precision_with_ref\" in existing_result and\n",
        "            not self.is_failed_metric(existing_result[\"context_precision_with_ref\"])):\n",
        "            context_score = existing_result[\"context_precision_with_ref\"]\n",
        "            print(f\"  context_precision_with_ref: {context_score:.4f} (reusing)\")\n",
        "        elif contexts:\n",
        "            try:\n",
        "                try:\n",
        "                    loop = asyncio.get_event_loop()\n",
        "                except RuntimeError:\n",
        "                    loop = asyncio.new_event_loop()\n",
        "                    asyncio.set_event_loop(loop)\n",
        "\n",
        "                context_score = loop.run_until_complete(\n",
        "                    self.evaluate_context_precision_with_ref(contexts, qa['query'], qa['generation_gt'][0])\n",
        "                )\n",
        "                print(f\"  context_precision_with_ref: {context_score:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                context_score = {\"error\": f\"Context precision evaluation error: {str(e)}\"}\n",
        "                print(f\"  Context precision evaluation failed: {e}\")\n",
        "        else:\n",
        "            context_score = {\"error\": \"No contexts available\"}\n",
        "            print(\"  context_precision_with_ref: No contexts available\")\n",
        "\n",
        "        result[\"context_precision_with_ref\"] = context_score\n",
        "\n",
        "        return result\n",
        "\n",
        "    def run_evaluation(self, rag_system, qa_pairs, config_name, start_index=0, retry_failed=False):\n",
        "        \"\"\"Run evaluation with auto-save after each question and optional retry of failed metrics\"\"\"\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_file = f\"eval_{config_name}_{timestamp}.json\"\n",
        "\n",
        "        results = []\n",
        "        existing_results_map = {}\n",
        "\n",
        "        if os.path.exists(results_file):\n",
        "            with open(results_file, 'r') as f:\n",
        "                results = json.load(f)\n",
        "            # Create map for quick lookup\n",
        "            existing_results_map = {r[\"id\"]: r for r in results}\n",
        "            if not retry_failed:\n",
        "                start_index = len(results)\n",
        "            print(f\"Loaded {len(results)} existing results\")\n",
        "\n",
        "        print(f\"Starting evaluation: {config_name}\")\n",
        "        print(f\"Results file: {results_file}\")\n",
        "        print(f\"Processing {len(qa_pairs)} questions from index {start_index}\")\n",
        "        if retry_failed:\n",
        "            failed_count = sum(1 for r in results if self.needs_retry(r))\n",
        "            print(f\"Retry mode: Will retry {failed_count} failed results\")\n",
        "\n",
        "        try:\n",
        "            for i in range(start_index, len(qa_pairs)):\n",
        "                qa = qa_pairs[i]\n",
        "\n",
        "                # Check if we should skip this question (already done and not retry mode)\n",
        "                existing_result = existing_results_map.get(i)\n",
        "                if existing_result and not retry_failed:\n",
        "                    print(f\"\\n--- Question {i+1}/{len(qa_pairs)} (skipping - already done) ---\")\n",
        "                    continue\n",
        "                elif existing_result and retry_failed and not self.needs_retry(existing_result):\n",
        "                    print(f\"\\n--- Question {i+1}/{len(qa_pairs)} (skipping - no retry needed) ---\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\n--- Waiting for Question {i+1}/{len(qa_pairs)}---\\n\")\n",
        "                time.sleep(15) # To prevent rate limits errors\n",
        "\n",
        "                try:\n",
        "                    result = self.evaluate_single_qa(rag_system, qa, i, existing_result)\n",
        "\n",
        "                    if i in existing_results_map:\n",
        "                        for j, r in enumerate(results):\n",
        "                            if r[\"id\"] == i:\n",
        "                                results[j] = result\n",
        "                                break\n",
        "                    else:\n",
        "                        results.append(result)\n",
        "\n",
        "                    # Save after each question\n",
        "                    with open(results_file, 'w') as f:\n",
        "                        json.dump(results, f, indent=2)\n",
        "\n",
        "                    print(f\"Saved progress: {len([r for r in results if not self.needs_retry(r)])}/{len(qa_pairs)} successful\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error on question {i+1}: {e}\")\n",
        "\n",
        "                    # Create or update error result\n",
        "                    error_result = {\n",
        "                        \"id\": i,\n",
        "                        \"question\": qa['query'],\n",
        "                        \"error\": str(e),\n",
        "                        \"timestamp\": datetime.now().isoformat()\n",
        "                    }\n",
        "\n",
        "                    if i in existing_results_map:\n",
        "                        for j, r in enumerate(results):\n",
        "                            if r[\"id\"] == i:\n",
        "                                results[j] = error_result\n",
        "                                break\n",
        "                    else:\n",
        "                        results.append(error_result)\n",
        "\n",
        "                    with open(results_file, 'w') as f:\n",
        "                        json.dump(results, f, indent=2)\n",
        "\n",
        "                    if input(\"Continue? (y/n): \").lower() != 'y':\n",
        "                        break\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(f\"\\nStopped. Progress saved to {results_file}\")\n",
        "\n",
        "        # Print summary\n",
        "        successful = len([r for r in results if not self.needs_retry(r)])\n",
        "        failed = len([r for r in results if self.needs_retry(r)])\n",
        "        print(f\"\\nCompleted: {successful} successful, {failed} failed out of {len(qa_pairs)} total questions\")\n",
        "\n",
        "        return results, results_file\n",
        "\n",
        "    def resume_evaluation(self, results_file, rag_system, qa_pairs, retry_failed=False):\n",
        "        \"\"\"Resume evaluation from existing results file\"\"\"\n",
        "        if not os.path.exists(results_file):\n",
        "            raise FileNotFoundError(f\"Results file {results_file} not found\")\n",
        "\n",
        "        with open(results_file, 'r') as f:\n",
        "            existing_results = json.load(f)\n",
        "\n",
        "        config_name = results_file.split('_')[1] if '_' in results_file else \"resumed\"\n",
        "\n",
        "        if retry_failed:\n",
        "            start_index = 0\n",
        "            print(f\"Retrying failed metrics from {results_file}\")\n",
        "        else:\n",
        "            # Normal resume mode, start from where we left off\n",
        "            start_index = len(existing_results)\n",
        "            print(f\"Resuming evaluation from question {start_index + 1}\")\n",
        "\n",
        "        # Copy the existing results file to preserve original\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        new_results_file = f\"eval_{config_name}_{timestamp}.json\"\n",
        "\n",
        "        with open(new_results_file, 'w') as f:\n",
        "            json.dump(existing_results, f, indent=2)\n",
        "\n",
        "        return self.run_evaluation(rag_system, qa_pairs, config_name, start_index, retry_failed)\n",
        "\n",
        "    def analyze_results(self, results_file: str, save_plot: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Analysis of RAGAS evaluation results with visualization\n",
        "        \"\"\"\n",
        "        with open(results_file, 'r') as f:\n",
        "            results = json.load(f)\n",
        "\n",
        "        data = []\n",
        "        for i, result in enumerate(results):\n",
        "            row = {\n",
        "                'id': result.get('id', i + 1),\n",
        "                'question_num': result.get('question_num', i + 1)\n",
        "            }\n",
        "\n",
        "            if 'metrics' in result:\n",
        "                row.update(self._extract_metrics(result['metrics']))\n",
        "            if 'context_precision' in result and isinstance(result['context_precision'], dict):\n",
        "                row.update(self._extract_context_precision(result['context_precision']))\n",
        "            if 'context_precision_with_ref' in result:\n",
        "                row.update(self._extract_context_precision_with_ref(result['context_precision_with_ref']))\n",
        "\n",
        "            data.append(row)\n",
        "\n",
        "        if not data:\n",
        "            print(\"No valid results found in the file.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        df = self._convert_numeric_columns(df)\n",
        "\n",
        "        metric_cols = self._get_metric_columns(df)\n",
        "        self._print_analysis(df, metric_cols)\n",
        "\n",
        "        if save_plot and metric_cols:\n",
        "            self._create_visualization(df, metric_cols, results_file)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def _extract_metrics(self, metrics: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract numeric metrics from metrics dictionary\"\"\"\n",
        "        extracted = {}\n",
        "        excluded = {'user_input', 'retrieved_contexts', 'response', 'reference'}\n",
        "\n",
        "        for key, value in metrics.items():\n",
        "            if key not in excluded:\n",
        "                try:\n",
        "                    if isinstance(value, (int, float)):\n",
        "                        extracted[key] = float(value)\n",
        "                    elif isinstance(value, str) and value.replace('.', '').replace('-', '').isdigit():\n",
        "                        extracted[key] = float(value) if value else np.nan\n",
        "                    elif pd.isna(value):\n",
        "                        extracted[key] = np.nan\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        return extracted\n",
        "\n",
        "\n",
        "    def _extract_context_precision(self, context_precision: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract context precision metrics\"\"\"\n",
        "        extracted = {}\n",
        "        for key, value in context_precision.items():\n",
        "            if key != 'error':\n",
        "                try:\n",
        "                    extracted[key] = float(value) if value else np.nan\n",
        "                except (ValueError, TypeError):\n",
        "                    extracted[key] = np.nan\n",
        "        return extracted\n",
        "\n",
        "\n",
        "    def _extract_context_precision_with_ref(self, context_precision_with_ref: Any) -> Dict[str, Any]:\n",
        "        \"\"\"Extract context precision with reference metrics\"\"\"\n",
        "        extracted = {}\n",
        "\n",
        "        if isinstance(context_precision_with_ref, (int, float)):\n",
        "            extracted['context_precision_with_ref'] = float(context_precision_with_ref)\n",
        "        elif isinstance(context_precision_with_ref, dict) and 'error' not in context_precision_with_ref:\n",
        "            try:\n",
        "                if len(context_precision_with_ref) == 1:\n",
        "                    extracted['context_precision_with_ref'] = float(list(context_precision_with_ref.values())[0])\n",
        "                else:\n",
        "                    for k, v in context_precision_with_ref.items():\n",
        "                        if k != 'error':\n",
        "                            extracted[f'context_precision_with_ref_{k}'] = float(v) if isinstance(v, (int, float)) else np.nan\n",
        "            except (ValueError, TypeError):\n",
        "                extracted['context_precision_with_ref'] = np.nan\n",
        "        else:\n",
        "            extracted['context_precision_with_ref'] = np.nan\n",
        "\n",
        "        return extracted\n",
        "\n",
        "\n",
        "    def _convert_numeric_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Convert columns to numeric where appropriate\"\"\"\n",
        "        for col in df.columns:\n",
        "            if col not in ['id', 'question_num']:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        return df\n",
        "\n",
        "\n",
        "    def _get_metric_columns(self, df: pd.DataFrame) -> List[str]:\n",
        "        \"\"\"Get list of metric columns (numeric columns excluding metadata)\"\"\"\n",
        "        excluded = {\n",
        "            'id', 'question_num', 'user_input', 'retrieved_contexts', 'response',\n",
        "            'reference', 'question', 'answer', 'ground_truth', 'contexts', 'error', 'timestamp'\n",
        "        }\n",
        "\n",
        "        metric_cols = []\n",
        "        for col in df.columns:\n",
        "            if col not in excluded:\n",
        "                try:\n",
        "                    pd.to_numeric(df[col], errors='coerce')\n",
        "                    metric_cols.append(col)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        return metric_cols\n",
        "\n",
        "\n",
        "    def _print_analysis(self, df: pd.DataFrame, metric_cols: List[str]) -> None:\n",
        "        \"\"\"Print comprehensive analysis results\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"RAGAS EVALUATION RESULTS ANALYSIS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Total Questions Evaluated: {len(df)}\")\n",
        "        print(f\"Metrics Evaluated: {', '.join(metric_cols)}\")\n",
        "\n",
        "        # Summary statistics\n",
        "        print(f\"\\n{'SUMMARY STATISTICS':<30}\")\n",
        "        print(f\"{'-'*60}\")\n",
        "\n",
        "        summary_data = []\n",
        "        for metric in metric_cols:\n",
        "            valid_data = df[metric].dropna()\n",
        "            n_valid = len(valid_data)\n",
        "            n_total = len(df)\n",
        "\n",
        "            if n_valid > 0:\n",
        "                stats = {\n",
        "                    'Metric': metric,\n",
        "                    'Count': f\"{n_valid}/{n_total}\",\n",
        "                    'Mean': f\"{valid_data.mean():.3f}\",\n",
        "                    'Std': f\"{valid_data.std():.3f}\",\n",
        "                    'Min': f\"{valid_data.min():.3f}\",\n",
        "                    'Max': f\"{valid_data.max():.3f}\",\n",
        "                    'Median': f\"{valid_data.median():.3f}\"\n",
        "                }\n",
        "            else:\n",
        "                stats = {\n",
        "                    'Metric': metric,\n",
        "                    'Count': f\"0/{n_total}\",\n",
        "                    'Mean': 'N/A',\n",
        "                    'Std': 'N/A',\n",
        "                    'Min': 'N/A',\n",
        "                    'Max': 'N/A',\n",
        "                    'Median': 'N/A'\n",
        "                }\n",
        "            summary_data.append(stats)\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        print(summary_df.to_string(index=False, justify='center'))\n",
        "\n",
        "        self._print_question_breakdown(df, metric_cols)\n",
        "        self._print_missing_data_analysis(df, metric_cols)\n",
        "\n",
        "\n",
        "    def _print_question_breakdown(self, df: pd.DataFrame, metric_cols: List[str]) -> None:\n",
        "        \"\"\"Print question-by-question breakdown\"\"\"\n",
        "        print(f\"\\n{'QUESTION-BY-QUESTION BREAKDOWN':<40}\")\n",
        "        print(f\"{'-'*60}\")\n",
        "\n",
        "        display_cols = ['question_num'] + metric_cols\n",
        "        display_df = df[display_cols].copy()\n",
        "\n",
        "        # Format numeric columns\n",
        "        for col in metric_cols:\n",
        "            display_df[col] = display_df[col].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\")\n",
        "\n",
        "        print(display_df.to_string(index=False))\n",
        "\n",
        "\n",
        "    def _print_missing_data_analysis(self, df: pd.DataFrame, metric_cols: List[str]) -> None:\n",
        "        \"\"\"Print missing data analysis\"\"\"\n",
        "        missing_data = df[metric_cols].isnull().sum()\n",
        "        if missing_data.sum() > 0:\n",
        "            print(f\"\\n{'MISSING DATA ANALYSIS':<30}\")\n",
        "            print(f\"{'-'*60}\")\n",
        "            for metric, missing_count in missing_data.items():\n",
        "                if missing_count > 0:\n",
        "                    missing_pct = (missing_count / len(df)) * 100\n",
        "                    print(f\"{metric:<25}: {missing_count}/{len(df)} ({missing_pct:.1f}%)\")\n",
        "\n",
        "\n",
        "    def _create_visualization(self, df: pd.DataFrame, metric_cols: List[str], results_file: str) -> None:\n",
        "        \"\"\"Create simplified visualization with mean scores and distribution plots\"\"\"\n",
        "\n",
        "        plt.style.use('default')\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        fig.suptitle('RAGAS Evaluation Results Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "        valid_means = []\n",
        "        valid_metrics = []\n",
        "        box_data = []\n",
        "        box_labels = []\n",
        "\n",
        "        for metric in metric_cols:\n",
        "            valid_data = df[metric].dropna()\n",
        "            if len(valid_data) > 0:\n",
        "                valid_means.append(valid_data.mean())\n",
        "                valid_metrics.append(metric.replace('_', '\\n'))\n",
        "                box_data.append(valid_data.values)\n",
        "                box_labels.append(metric.replace('_', '\\n'))\n",
        "\n",
        "        # Plot 1: Mean scores by metric\n",
        "        if valid_means:\n",
        "            colors = ['#2E86C1', '#28B463', '#F39C12', '#E74C3C', '#8E44AD', '#17A2B8']\n",
        "            bars = ax1.bar(range(len(valid_means)), valid_means,\n",
        "                          color=colors[:len(valid_means)])\n",
        "\n",
        "            ax1.set_title('Mean Scores by Metric', fontsize=14, fontweight='bold')\n",
        "            ax1.set_xticks(range(len(valid_means)))\n",
        "            ax1.set_xticklabels(valid_metrics, rotation=45, ha='right')\n",
        "            ax1.set_ylabel('Score')\n",
        "            ax1.set_ylim(0, 1.1)\n",
        "            ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for bar, mean_val in zip(bars, valid_means):\n",
        "                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                        f'{mean_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        # Plot 2: Score distribution by metric\n",
        "        if box_data:\n",
        "            bp = ax2.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
        "\n",
        "            # Color the boxes\n",
        "            colors = ['#2E86C1', '#28B463', '#F39C12', '#E74C3C', '#8E44AD', '#17A2B8']\n",
        "            for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):\n",
        "                patch.set_facecolor(color)\n",
        "                patch.set_alpha(0.7)\n",
        "\n",
        "            ax2.set_title('Score Distribution by Metric', fontsize=14, fontweight='bold')\n",
        "            ax2.set_ylabel('Score')\n",
        "            ax2.tick_params(axis='x', rotation=45)\n",
        "            ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        plot_filename = results_file.replace('.json', '_analysis.png')\n",
        "        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\nVisualization saved as: {plot_filename}\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ioCFOF7lCeaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class RAGConfig:\n",
        "    \"\"\"Configuration class for RAG system\"\"\"\n",
        "    api_key: str\n",
        "    indexer: Dict[str, Any]\n",
        "    preretrieval: Union[bool, Dict[str, Any]] = False\n",
        "    retriever: Dict[str, Any] = None\n",
        "    postretriever: Union[bool, Dict[str, Any]] = False\n",
        "    generator: Dict[str, Any] = None\n",
        "    orchestration: Union[bool, Dict[str, Any]] = False\n",
        "    iterations: Union[bool, Dict[str, Any]] = False\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, config_dict: Dict[str, Any]):\n",
        "        return cls(**config_dict)\n",
        "\n",
        "    @classmethod\n",
        "    def from_json(cls, json_path: str):\n",
        "        with open(json_path, 'r') as f:\n",
        "            config_dict = json.load(f)\n",
        "        return cls.from_dict(config_dict)\n",
        "\n",
        "class ConfigurableRAGSystem:\n",
        "    def __init__(self, config: Union[RAGConfig, Dict[str, Any], str]):\n",
        "        \"\"\"\n",
        "        Initialize RAG system with configuration\n",
        "        \"\"\"\n",
        "        if isinstance(config, str):\n",
        "            self.config = RAGConfig.from_json(config)\n",
        "        elif isinstance(config, dict):\n",
        "            self.config = RAGConfig.from_dict(config)\n",
        "        else:\n",
        "            self.config = config\n",
        "\n",
        "        self._initialize_components()\n",
        "\n",
        "    def _initialize_components(self):\n",
        "        \"\"\"Initialize all components based on configuration\"\"\"\n",
        "\n",
        "        self.indexer = self._create_indexer()\n",
        "        self.preretrieval = self._create_preretrieval() if self.config.preretrieval else None\n",
        "        self.retriever = self._create_retriever()\n",
        "        self.postretrieval = self._create_postretrieval() if self.config.postretriever else None\n",
        "        self.generator = self._create_generator()\n",
        "        self.orchestration = self._create_orchestration() if self.config.orchestration else None\n",
        "\n",
        "    def _create_indexer(self):\n",
        "        \"\"\"Create indexer based on configuration\"\"\"\n",
        "        indexer_config = self.config.indexer\n",
        "        method = indexer_config.get('method', 'setup_vector_store_from_arxiv')\n",
        "\n",
        "        indexer = Indexer(\n",
        "            embedding_model=indexer_config.get('embedding', 'sentence-transformers/all-MiniLM-L6-v2'),\n",
        "            chunk_size=indexer_config.get('chunk_size', 512),\n",
        "            overlap=indexer_config.get('overlap', 50),\n",
        "            persist_dir=indexer_config.get('persist_dir', './chroma_db')\n",
        "        )\n",
        "\n",
        "        if method == 'setup_vector_store_from_arxiv':\n",
        "            indexer.setup_vector_store_from_arxiv()\n",
        "        elif method == 'setup_vector_store':\n",
        "            indexer.setup_vector_store()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown indexer method: {method}\")\n",
        "\n",
        "        return indexer\n",
        "\n",
        "    def _create_preretrieval(self):\n",
        "        \"\"\"Create pre-retrieval module based on configuration\"\"\"\n",
        "        if not self.config.preretrieval:\n",
        "            return None\n",
        "\n",
        "        preretrieval_config = self.config.preretrieval\n",
        "        return PreRetrieval(\n",
        "            model=preretrieval_config.get('model', 'gemini-2.5-flash'),\n",
        "            gemini_api_key=self.config.api_key\n",
        "        )\n",
        "\n",
        "    def _create_retriever(self):\n",
        "        \"\"\"Create retriever based on configuration\"\"\"\n",
        "        retriever_config = self.config.retriever or {}\n",
        "        return Retriever(\n",
        "            self.indexer,\n",
        "            top_k=retriever_config.get('top_k', 5)\n",
        "        )\n",
        "\n",
        "    def _create_postretrieval(self):\n",
        "        \"\"\"Create post-retrieval module based on configuration\"\"\"\n",
        "        if not self.config.postretriever:\n",
        "            return None\n",
        "\n",
        "        postretriever_config = self.config.postretriever\n",
        "        return PostRetrieval(\n",
        "            model=postretriever_config.get('model', 'BAAI/bge-reranker-base'),\n",
        "            num_to_return=postretriever_config.get('num_to_return', 5)\n",
        "        )\n",
        "\n",
        "    def _create_generator(self):\n",
        "        \"\"\"Create generator based on configuration\"\"\"\n",
        "        generator_config = self.config.generator or {}\n",
        "        return Generator(\n",
        "            model=generator_config.get('model', 'gemini-2.5-flash'),\n",
        "            gemini_api_key=self.config.api_key\n",
        "        )\n",
        "\n",
        "    def _create_orchestration(self):\n",
        "        \"\"\"Create orchestration module based on configuration\"\"\"\n",
        "        if not self.config.orchestration:\n",
        "            return None\n",
        "\n",
        "        orchestration_config = self.config.orchestration\n",
        "        return OrchestrationModule(\n",
        "            model=orchestration_config.get('model', 'gemini-2.5-flash'),\n",
        "            gemini_api_key=self.config.api_key\n",
        "        )\n",
        "\n",
        "    def process_query(self, query: str) -> tuple:\n",
        "        \"\"\"\n",
        "        Process query based on configuration settings\n",
        "        \"\"\"\n",
        "        if self.config.iterations:\n",
        "            iteration_config = self.config.iterations if isinstance(self.config.iterations, dict) else {}\n",
        "            flow_type = iteration_config.get('type', 'iterative')\n",
        "\n",
        "            if flow_type == 'iterative':\n",
        "                return self._iterative_search_flow(query)\n",
        "            elif flow_type == 'recursive':\n",
        "                return self._recursive_search_flow(query)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown iteration flow type: {flow_type}\")\n",
        "        else:\n",
        "            return self._advanced_rag_flow(query)\n",
        "\n",
        "    def _iterative_search_flow(self, original_query: str) -> tuple:\n",
        "        \"\"\"\n",
        "        Iterative search flow: refining query each iteration\n",
        "        \"\"\"\n",
        "        iteration_config = self.config.iterations if isinstance(self.config.iterations, dict) else {}\n",
        "        max_iterations = iteration_config.get('max_iterations', 3)\n",
        "\n",
        "        accumulated_contexts = []\n",
        "        current_query = original_query\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            print(f\"\\n--- Iteration {iteration + 1} ---\")\n",
        "            new_contexts = self._perform_enhanced_retrieval(current_query)\n",
        "            accumulated_contexts.extend(new_contexts)\n",
        "\n",
        "            if self.postretrieval and len(accumulated_contexts)>self.postretrieval.num_to_return:\n",
        "                top_contexts = self.postretrieval.rerank(original_query, accumulated_contexts)\n",
        "            else:\n",
        "                top_contexts = accumulated_contexts\n",
        "\n",
        "            answer = self.generator.generate(original_query, top_contexts)\n",
        "\n",
        "            # Judge if we need more information\n",
        "            if self.orchestration and self.orchestration.judge_answer_completeness(original_query, answer):\n",
        "                print(f\"Information complete after {iteration + 1} iterations\")\n",
        "                return answer, top_contexts\n",
        "\n",
        "            if iteration < max_iterations - 1:  # Don't refine on last iteration\n",
        "                current_query = self.orchestration.refine_query_recursively(original_query, current_query, answer, iteration)\n",
        "                print(f\"Refined query: {current_query}\")\n",
        "\n",
        "        return answer, top_contexts\n",
        "\n",
        "    def _recursive_search_flow(self, query: str, depth: int = 0) -> tuple:\n",
        "        \"\"\"\n",
        "        Recursive search flow: decompose complex queries into sub-problems\n",
        "        and solve them recursively, then synthesize results\n",
        "        \"\"\"\n",
        "        iteration_config = self.config.iterations if isinstance(self.config.iterations, dict) else {}\n",
        "        max_depth = iteration_config.get('max_depth', 3)\n",
        "        max_sub_queries = iteration_config.get('max_sub_queries', 3)\n",
        "\n",
        "        print(f\"\\n{'  ' * depth}=== Recursive RAG - Depth {depth} ===\")\n",
        "        print(f\"{'  ' * depth}Query: {query}\")\n",
        "\n",
        "        # Base case: if we've reached max depth, use simple RAG\n",
        "        if depth >= max_depth:\n",
        "            print(f\"{'  ' * depth}Max depth reached, using simple RAG\")\n",
        "            return self._advanced_rag_flow(query)\n",
        "\n",
        "        # Step 1: Try to answer the query directly first\n",
        "        direct_contexts = self._perform_enhanced_retrieval(query)\n",
        "        direct_answer = self.generator.generate(query, direct_contexts)\n",
        "        print(f\"{'  ' * depth}Direct answer attempt: {direct_answer}\")\n",
        "\n",
        "        # Step 2: Judge if direct answer is sufficient\n",
        "        if self.orchestration and self.orchestration.judge_answer_completeness(query, direct_answer):\n",
        "            print(f\"{'  ' * depth}Direct answer is sufficient\")\n",
        "            return direct_answer, direct_contexts\n",
        "\n",
        "        # Step 3: Decompose query into sub-problems using orchestration\n",
        "        print(f\"{'  ' * depth}Direct answer insufficient, decomposing query...\")\n",
        "        if self.preretrieval:\n",
        "            sub_queries = self.preretrieval.decompose_query(query, direct_answer, max_sub_queries)\n",
        "\n",
        "        if not sub_queries or len(sub_queries) == 0:\n",
        "            print(f\"{'  ' * depth}Could not decompose query, returning direct answer\")\n",
        "            return direct_answer, direct_contexts\n",
        "\n",
        "        print(f\"{'  ' * depth}Sub-queries: {sub_queries}\")\n",
        "\n",
        "        # Step 4: Recursively solve each sub-problem\n",
        "        sub_results = []\n",
        "        all_contexts = direct_contexts.copy()\n",
        "\n",
        "        for i, sub_query in enumerate(sub_queries):\n",
        "            print(f\"{'  ' * depth}Solving sub-query {i+1}/{len(sub_queries)}: {sub_query}\")\n",
        "\n",
        "            try:\n",
        "                # Recursive call with increased depth\n",
        "                sub_answer, sub_contexts = self._recursive_search_flow(sub_query, depth + 1)\n",
        "                sub_results.append({\n",
        "                    'query': sub_query,\n",
        "                    'answer': sub_answer,\n",
        "                    'contexts': sub_contexts\n",
        "                })\n",
        "\n",
        "                # Accumulate contexts from sub-queries\n",
        "                if sub_contexts:\n",
        "                    all_contexts.extend(sub_contexts)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"{'  ' * depth}Error solving sub-query '{sub_query}': {e}\")\n",
        "                # Continue with other sub-queries even if one fails\n",
        "                continue\n",
        "\n",
        "        # Step 5: Synthesize results from sub-problems using orchestration\n",
        "        if sub_results:\n",
        "            print(f\"{'  ' * depth}Synthesizing results from {len(sub_results)} sub-queries...\")\n",
        "\n",
        "            if self.postretrieval and len(all_contexts) > 5:\n",
        "                final_contexts = self.postretrieval.rerank(query, all_contexts)\n",
        "\n",
        "            # Synthesize final answer using orchestration module\n",
        "            final_answer = self.orchestration.synthesize_recursive_answer(query, direct_answer, sub_results)\n",
        "\n",
        "            print(f\"{'  ' * depth}Final synthesized answer: {final_answer}...\")\n",
        "            return final_answer, final_contexts\n",
        "        else:\n",
        "            print(f\"{'  ' * depth}No successful sub-results, returning direct answer\")\n",
        "            return direct_answer, direct_contexts\n",
        "\n",
        "\n",
        "    def _naive_rag_flow(self, query: str) -> tuple:\n",
        "        \"\"\"Simple RAG flow without orchestration\"\"\"\n",
        "\n",
        "        results = self._perform_retrieval(query)\n",
        "        answer = self.generator.generate(query, results)\n",
        "\n",
        "        return answer, results\n",
        "\n",
        "    def _advanced_rag_flow(self, query: str) -> tuple:\n",
        "        \"\"\"Advanced RAG flow with orchestration\"\"\"\n",
        "        # Check if retrieval is necessary\n",
        "        if self.orchestration and self.config.orchestration.get('judge_retrieval_necessity', False):\n",
        "            if not self.orchestration.judge_retrieval_necessity(query):\n",
        "                answer = self.generator.generate(query, None)\n",
        "                return answer, None\n",
        "\n",
        "        # Perform retrieval with enhancements\n",
        "        results = self._perform_enhanced_retrieval(query)\n",
        "\n",
        "        # Generate answer\n",
        "        answer = self.generator.generate(query, results)\n",
        "\n",
        "        return answer, results\n",
        "\n",
        "    def _perform_retrieval(self, query: str) -> List:\n",
        "        \"\"\"Perform basic retrieval based on configuration\"\"\"\n",
        "        retriever_config = self.config.retriever or {}\n",
        "        method = retriever_config.get('method', 'dense_retrieve')\n",
        "\n",
        "        if method == 'dense_retrieve':\n",
        "            return self.retriever.dense_retrieve(query)\n",
        "        elif method == 'sparse_retrieve':\n",
        "            return self.retriever.sparse_retrieve(query)\n",
        "        elif isinstance(method, dict) and 'hybrid_retrieve' in method:\n",
        "            alpha = method['hybrid_retrieve']\n",
        "            return self.retriever.hybrid_retrieve(query, alpha=alpha)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown retrieval method: {method}\")\n",
        "\n",
        "    def _perform_enhanced_retrieval(self, query: str) -> List:\n",
        "        \"\"\"Perform enhanced retrieval with pre/post processing\"\"\"\n",
        "        processed_query = query\n",
        "        if self.preretrieval and isinstance(self.config.preretrieval, dict):\n",
        "            if self.config.preretrieval.get('rewrite_query', False):\n",
        "                processed_query = self.preretrieval.rewrite_query(query)\n",
        "            if self.config.preretrieval.get('hyde', False):\n",
        "                processed_query = self.preretrieval.hyde(query)\n",
        "\n",
        "        all_results = self._perform_retrieval(processed_query)\n",
        "\n",
        "        # Decompose query and retrieve for sub-queries\n",
        "        if self.preretrieval and isinstance(self.config.preretrieval, dict):\n",
        "            if self.config.preretrieval.get('decompose_query', False):\n",
        "                decomposed_queries = self.preretrieval.decompose_query(processed_query)\n",
        "                for sub_query in decomposed_queries:\n",
        "                    sub_results = self._perform_retrieval(sub_query)\n",
        "                    all_results.extend(sub_results)\n",
        "\n",
        "        # Web search if needed\n",
        "        if self.orchestration and self.config.orchestration.get('judge_query_needs_web_search', False):\n",
        "            if self.orchestration.judge_query_needs_web_search(query):\n",
        "                web_results = self.retriever.web_search_retrieve(\n",
        "                    query, use_duckduckgo=True, use_wikipedia=True, top_k=5\n",
        "                )\n",
        "                all_results.extend(web_results)\n",
        "\n",
        "        if self.postretrieval:\n",
        "            all_results = self.postretrieval.rerank(query, all_results)\n",
        "\n",
        "        return all_results"
      ],
      "metadata": {
        "id": "2xXdc7qaCgOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration examples\n",
        "def create_naive_config():\n",
        "    \"\"\"Create a naive RAG configuration\"\"\"\n",
        "    return {\n",
        "        \"api_key\": \"your_api_key_here\",\n",
        "        \"indexer\": {\n",
        "            \"method\": \"setup_vector_store\",\n",
        "            \"embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            \"chunk_size\": 512,\n",
        "            \"overlap\": 50,\n",
        "            \"persist_dir\": \"./chroma_db\"\n",
        "        },\n",
        "        \"preretrieval\": False,\n",
        "        \"retriever\": {\n",
        "            \"method\": 'sparse_retrieve',\n",
        "            \"top_k\": 5\n",
        "        },\n",
        "        \"postretriever\": False,\n",
        "        \"generator\": {\n",
        "            \"model\": \"gemini-2.5-flash\"\n",
        "        },\n",
        "        \"orchestration\": False,\n",
        "        \"iterations\": False\n",
        "    }\n",
        "\n",
        "def create_advanced_config():\n",
        "    \"\"\"Create an advanced RAG configuration\"\"\"\n",
        "    return {\n",
        "        \"api_key\": \"your_api_key_here\",\n",
        "        \"indexer\": {\n",
        "            \"method\": \"setup_vector_store_from_arxiv\",\n",
        "            \"embedding\": \"NovaSearch/stella_en_400M_v5\",\n",
        "            \"chunk_size\": 512,\n",
        "            \"overlap\": 50,\n",
        "            \"persist_dir\": \"./chroma_db\"\n",
        "        },\n",
        "        \"preretrieval\": {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"rewrite_query\": True,\n",
        "            \"decompose_query\": False,\n",
        "            \"hyde\": False\n",
        "        },\n",
        "        \"retriever\": {\n",
        "            \"method\": {\"hybrid_retrieve\": 0.3},\n",
        "            \"top_k\": 30\n",
        "        },\n",
        "        \"postretriever\": {\n",
        "            \"model\": \"BAAI/bge-reranker-base\",\n",
        "            \"num_to_return\": 5\n",
        "        },\n",
        "        \"generator\": {\n",
        "            \"model\": \"gemini-2.5-flash\"\n",
        "        },\n",
        "        \"orchestration\": {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"judge_retrieval_necessity\": True,\n",
        "            \"judge_context_sufficiency\": True,\n",
        "            \"judge_query_needs_web_search\": True\n",
        "        },\n",
        "        \"iterations\": False\n",
        "    }\n",
        "\n",
        "def create_iterative_config():\n",
        "    \"\"\"Create a configuration with iterative search\"\"\"\n",
        "    return {\n",
        "        \"api_key\": \"your_api_key_here\",\n",
        "        \"indexer\": {\n",
        "            \"method\": \"setup_vector_store_from_arxiv\",\n",
        "            \"embedding\": \"NovaSearch/stella_en_400M_v5\",\n",
        "            \"chunk_size\": 512,\n",
        "            \"overlap\": 50,\n",
        "            \"persist_dir\": \"./chroma_db\"\n",
        "        },\n",
        "        \"preretrieval\": {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"rewrite_query\": False,\n",
        "            \"decompose_query\": False,\n",
        "            \"hyde\": True\n",
        "        },\n",
        "        \"retriever\": {\n",
        "            \"method\": 'sparse_retrieve',\n",
        "            \"top_k\": 20\n",
        "        },\n",
        "        \"postretriever\": {\n",
        "            \"model\": \"BAAI/bge-reranker-large\",\n",
        "            \"num_to_return\": 10\n",
        "        },\n",
        "        \"generator\": {\n",
        "            \"model\": \"gemini-2.5-flash\"\n",
        "        },\n",
        "        \"orchestration\": {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"judge_retrieval_necessity\": False,\n",
        "            \"judge_context_sufficiency\": False,\n",
        "            \"judge_query_needs_web_search\": False\n",
        "        },\n",
        "        \"iterations\": {\n",
        "            \"type\": \"iterative\",\n",
        "            \"max_iterations\": 5\n",
        "        }\n",
        "    }\n",
        "\n",
        "def create_recursive_config():\n",
        "    \"\"\"Create a configuration with recursive search\"\"\"\n",
        "    return {\n",
        "        \"api_key\": \"your_api_key_here\",\n",
        "        \"indexer\": {\n",
        "            \"method\": \"setup_vector_store_from_arxiv\",\n",
        "            \"embedding\": \"NovaSearch/stella_en_400M_v5\",\n",
        "            \"chunk_size\": 512,\n",
        "            \"overlap\": 50,\n",
        "            \"persist_dir\": \"./chroma_db\"\n",
        "        },\n",
        "        \"preretrieval\": {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"rewrite_query\": False,\n",
        "            \"decompose_query\": False,\n",
        "            \"hyde\": False\n",
        "        },\n",
        "        \"retriever\": {\n",
        "            \"method\": 'sparse_retrieve',\n",
        "            \"top_k\": 10\n",
        "        },\n",
        "        \"postretriever\": {\n",
        "            \"model\": \"BAAI/bge-reranker-large\",\n",
        "            \"num_to_return\": 5\n",
        "        },\n",
        "        \"generator\": {\n",
        "            \"model\": \"gemini-2.5-flash\"\n",
        "        },\n",
        "        \"orchestration\": {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"judge_retrieval_necessity\": False,\n",
        "            \"judge_context_sufficiency\": False,\n",
        "            \"judge_query_needs_web_search\": False\n",
        "        },\n",
        "        \"iterations\": {\n",
        "            \"type\": \"recursive\",\n",
        "            \"max_depth\": 3,\n",
        "            \"max_sub_queries\": 3\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "vy92m0zlFMZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGMainApplication:\n",
        "    \"\"\"Main application class for RAG system\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.supported_configs = {\n",
        "            'naive': create_naive_config,\n",
        "            'advanced': create_advanced_config,\n",
        "            'iterative': create_iterative_config,\n",
        "            'recursive': create_recursive_config\n",
        "        }\n",
        "\n",
        "    def load_configuration(self, config_name: str, custom_config: Dict[str, Any] = None, api_key: str = None, mode: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"Load RAG configuration based on parameters\"\"\"\n",
        "        if custom_config:\n",
        "            print(\"Using custom configuration provided\")\n",
        "            config = custom_config.copy()\n",
        "        else:\n",
        "            print(f\"Using predefined configuration: {config_name}\")\n",
        "            config_func = self.supported_configs[config_name]\n",
        "            config = config_func()\n",
        "\n",
        "\n",
        "        if mode == 'eval' and config['indexer']['method']!='setup_vector_store_from_arxiv':\n",
        "            config['indexer']['method'] = 'setup_vector_store_from_arxiv'\n",
        "            print(\"  → Adjusted indexer method to 'setup_vector_store_from_arxiv' for evaluation mode\")\n",
        "\n",
        "\n",
        "        # Update API key\n",
        "        if api_key:\n",
        "            config['api_key'] = api_key\n",
        "\n",
        "        return config\n",
        "\n",
        "    def run_pdf_extraction(self, input_dir, output_dir, api_key, max_workers=4, extract_images=True):\n",
        "        \"\"\"Run PDF extraction\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"PDF EXTRACTION\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Input directory: {input_dir}\")\n",
        "        print(f\"Output directory: {output_dir}\")\n",
        "        print(f\"Extract images: {extract_images}\")\n",
        "        print(f\"Max workers: {max_workers}\")\n",
        "\n",
        "        if not os.path.exists(input_dir):\n",
        "            print(f\"Error: Input directory '{input_dir}' does not exist\")\n",
        "            return False\n",
        "\n",
        "        # Check for PDF files\n",
        "        pdf_files = [f for f in os.listdir(input_dir) if f.endswith('.pdf')]\n",
        "        if not pdf_files:\n",
        "            print(f\"No PDF files found in '{input_dir}'\")\n",
        "            return False\n",
        "\n",
        "        print(f\"Found {len(pdf_files)} PDF files to process\")\n",
        "\n",
        "        try:\n",
        "            extractor = PDFInfoExtractor(\n",
        "                input_dir=input_dir,\n",
        "                output_dir=output_dir,\n",
        "                api_key=api_key,\n",
        "                desc_length=200\n",
        "            )\n",
        "\n",
        "            start_time = time.time()\n",
        "            extractor.process_all_pdfs(\n",
        "                max_workers=max_workers,\n",
        "                extract_images=extract_images\n",
        "            )\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"\\n✅ PDF extraction completed in {elapsed_time:.2f} seconds\")\n",
        "            print(f\"Results saved to: {output_dir}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during PDF extraction: {e}\")\n",
        "            return False\n",
        "\n",
        "    def run_evaluation_mode(self, config_name, custom_config, api_key, qa_file, evaluator_api_key=None,\n",
        "                          resume_file=None, retry_failed=False, start_index=0):\n",
        "        \"\"\"Run evaluation mode\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"EVALUATION MODE\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Load configuration\n",
        "        try:\n",
        "            config = self.load_configuration(config_name, custom_config, api_key, mode='eval')\n",
        "            print(f\"Configuration loaded: {config_name if not custom_config else 'custom'}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading configuration: {e}\")\n",
        "            return False\n",
        "\n",
        "        # Load QA data\n",
        "        if resume_file:\n",
        "            print(f\"Resuming evaluation from: {resume_file}\")\n",
        "            qa_pairs = self.load_qa_data(qa_file) if qa_file else []\n",
        "        else:\n",
        "            if not qa_file:\n",
        "                print(\"Error: QA file is required for new evaluation\")\n",
        "                return False\n",
        "            qa_pairs = self.load_qa_data(qa_file)\n",
        "\n",
        "        if not qa_pairs and not resume_file:\n",
        "            print(\"Error: No QA pairs loaded\")\n",
        "            return False\n",
        "\n",
        "        print(f\"Loaded {len(qa_pairs)} QA pairs\")\n",
        "\n",
        "        # Initialize systems\n",
        "        try:\n",
        "            print(\"Initializing RAG system...\")\n",
        "            rag_system = ConfigurableRAGSystem(config)\n",
        "\n",
        "            print(\"Initializing evaluator...\")\n",
        "            eval_api_key = evaluator_api_key or api_key\n",
        "            evaluator = Evaluator(\n",
        "                llm_model=\"gemini-2.5-flash\",\n",
        "                api_key=eval_api_key\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing systems: {e}\")\n",
        "            return False\n",
        "\n",
        "        # Run evaluation\n",
        "        try:\n",
        "            eval_config_name = config_name if not custom_config else \"custom\"\n",
        "\n",
        "            if resume_file:\n",
        "                results, results_file = evaluator.resume_evaluation(\n",
        "                    resume_file, rag_system, qa_pairs,\n",
        "                    retry_failed=retry_failed\n",
        "                )\n",
        "            else:\n",
        "                results, results_file = evaluator.run_evaluation(\n",
        "                    rag_system, qa_pairs, eval_config_name,\n",
        "                    start_index=start_index\n",
        "                )\n",
        "\n",
        "            print(f\"\\n✅ Evaluation completed!\")\n",
        "            print(f\"Results saved to: {results_file}\")\n",
        "\n",
        "            evaluator.analyze_results(results_file)\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during evaluation: {e}\")\n",
        "            return False\n",
        "\n",
        "    def run_app_mode(self, config_name, custom_config, api_key, data_dir, pdf_input_dir=None,\n",
        "                    max_workers=4, extract_images=True):\n",
        "        \"\"\"Run interactive app mode with optional PDF extraction\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"INTERACTIVE APP MODE\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Check if PDF extraction is needed\n",
        "        if pdf_input_dir and os.path.exists(pdf_input_dir):\n",
        "            pdf_files = [f for f in os.listdir(pdf_input_dir) if f.endswith('.pdf')]\n",
        "            if pdf_files:\n",
        "                print(f\"Found {len(pdf_files)} PDF files. Starting extraction...\")\n",
        "                success = self.run_pdf_extraction(\n",
        "                    input_dir=pdf_input_dir,\n",
        "                    output_dir=data_dir,\n",
        "                    api_key=api_key,\n",
        "                    max_workers=max_workers,\n",
        "                    extract_images=extract_images\n",
        "                )\n",
        "                if not success:\n",
        "                    print(\"❌ PDF extraction failed\")\n",
        "                    return False\n",
        "\n",
        "        # Check for data\n",
        "        if not self.check_app_data(data_dir):\n",
        "            return False\n",
        "\n",
        "        # Load configuration\n",
        "        try:\n",
        "            config = self.load_configuration(config_name, custom_config, api_key, mode='app')\n",
        "            print(f\"Configuration loaded: {config_name if not custom_config else 'custom'}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading configuration: {e}\")\n",
        "            return False\n",
        "\n",
        "        return self.run_cli_interface(config, data_dir)\n",
        "\n",
        "    def check_app_data(self, data_dir):\n",
        "        \"\"\"Check if required data exists for app mode\"\"\"\n",
        "        extracted_data_file = os.path.join(data_dir, \"extracted_data.json\")\n",
        "\n",
        "        if os.path.exists(extracted_data_file):\n",
        "            print(f\"✅ Found extracted data: {extracted_data_file}\")\n",
        "            return True\n",
        "\n",
        "        print(f\"❌ No extracted data found in {data_dir}\")\n",
        "        print(\"   Please ensure you have 'extracted_data.json' file with processed PDFs\")\n",
        "        print(\"   or provide PDF_INPUT_DIR to extract PDFs automatically\")\n",
        "        return False\n",
        "\n",
        "    def run_cli_interface(self, config, data_dir):\n",
        "        \"\"\"Run command line interface\"\"\"\n",
        "        try:\n",
        "            print(\"Initializing RAG system...\")\n",
        "            rag_system = ConfigurableRAGSystem(config)\n",
        "\n",
        "            print(\"\\n✅ RAG system ready!\")\n",
        "            print(\"Ask questions about your documents. Type 'quit' to exit.\\n\")\n",
        "\n",
        "            while True:\n",
        "                try:\n",
        "                    question = input(\"❓ Your question: \").strip()\n",
        "\n",
        "                    if question.lower() in ['quit', 'exit', 'q']:\n",
        "                        print(\"Goodbye!\")\n",
        "                        break\n",
        "\n",
        "                    if not question:\n",
        "                        continue\n",
        "\n",
        "                    print(\"🤔 Processing...\")\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    answer, contexts = rag_system.process_query(question)\n",
        "\n",
        "                    elapsed_time = time.time() - start_time\n",
        "\n",
        "                    print(f\"\\n🤖 Answer:\")\n",
        "                    print(f\"{answer}\")\n",
        "\n",
        "                    if contexts:\n",
        "                        print(f\"\\n📚 Sources ({len(contexts)} contexts used):\")\n",
        "                        for i, context in enumerate(contexts, 1):\n",
        "                            if hasattr(context, 'metadata') and context.metadata:\n",
        "                                file_name = context.metadata.get('file_name', 'Unknown')\n",
        "                                title = context.metadata.get('title', 'Unknown')\n",
        "                                author = context.metadata.get('author', 'Unknown')\n",
        "                                page = context.metadata.get('page_count', '')\n",
        "                                page_info = f\" (Doc {page})\" if page else \"\"\n",
        "                                print(f\"  {i}. {file_name} [{author} {title}] {page_info}: {repr(context.text[:100])}...{repr(context.text[-100:])}\")\n",
        "                            else:\n",
        "                                print(f\"  {i}. Context {i}: {repr(context.text[:100])}...{repr(context.text[-100:])}\")\n",
        "\n",
        "                    print(f\"\\n⏱️  Response time: {elapsed_time:.2f}s\")\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                except KeyboardInterrupt:\n",
        "                    print(\"\\nGoodbye!\")\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error processing question: {e}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in CLI interface: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_qa_data(self, qa_file):\n",
        "        \"\"\"Load QA pairs from file\"\"\"\n",
        "        try:\n",
        "            if qa_file.endswith('.parquet'):\n",
        "                qa_df = pd.read_parquet(qa_file)\n",
        "                return qa_df[['query', 'generation_gt']].to_dict(orient='records')\n",
        "            elif qa_file.endswith('.json'):\n",
        "                with open(qa_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            else:\n",
        "                raise ValueError(\"QA file must be .parquet or .json format\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading QA data: {e}\")\n",
        "            return []"
      ],
      "metadata": {
        "id": "XlIZvtFoFQ0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Marker-Inc-Korea/AutoRAG_ARAGOG_Paper.git"
      ],
      "metadata": {
        "id": "qafLG_aEFThK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    🚀 RAG System Main Function\n",
        "\n",
        "    Configure the variables below and run this script.\n",
        "    Change MODE to switch between 'eval' or 'app' operations.\n",
        "    \"\"\"\n",
        "\n",
        "    # =============================================================================\n",
        "    # 📝 CONFIGURATION VARIABLES - EDIT THESE\n",
        "    # =============================================================================\n",
        "\n",
        "    # 🎯 OPERATION MODE - Choose one:\n",
        "    MODE = 'eval'  # 'eval' or 'app'\n",
        "\n",
        "    # 🔐 API KEY (Required)\n",
        "    API_KEY = \"AIzaSyCsh6Zuqs0DD4KUka9KfyyT5rMJY3GJcaQ\"\n",
        "\n",
        "    # ⚙️ RAG CONFIGURATION\n",
        "    # Option 1: Use predefined config\n",
        "    CONFIG_NAME = 'naive'  # 'naive', 'advanced', 'iterative', 'recursive'\n",
        "\n",
        "    # Option 2: Use custom config (uncomment set to None below to use CONFIG_NAME instead)\n",
        "    CUSTOM_CONFIG = {\n",
        "        \"api_key\": API_KEY,\n",
        "        \"indexer\": {\n",
        "            \"method\": \"setup_vector_store\", #\"setup_vector_store\" for app mode and \"setup_vector_store_from_arxiv\" for evaluation\n",
        "            \"embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            \"chunk_size\": 512,\n",
        "            \"overlap\": 50,\n",
        "            \"persist_dir\": \"./chroma_db\"\n",
        "        },\n",
        "        \"preretrieval\": {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"rewrite_query\": True,\n",
        "            \"decompose_query\": False,\n",
        "            \"hyde\": True\n",
        "        },\n",
        "        \"retriever\": {\n",
        "            \"method\": {\"hybrid_retrieve\": 0.3},\n",
        "            \"top_k\": 30\n",
        "        },\n",
        "        \"postretriever\": {\n",
        "            \"model\": \"BAAI/bge-reranker-base\",\n",
        "            \"num_to_return\": 5\n",
        "        },\n",
        "        \"generator\": {\n",
        "            \"model\": \"gemini-2.5-flash\"\n",
        "        },\n",
        "        \"orchestration\": {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"judge_retrieval_necessity\": True,\n",
        "            \"judge_context_sufficiency\": True,\n",
        "            \"judge_query_needs_web_search\": True\n",
        "        },\n",
        "        \"iterations\": False\n",
        "    }\n",
        "    # Set to None to use CONFIG_NAME instead:\n",
        "    # CUSTOM_CONFIG = None\n",
        "\n",
        "    # =============================================================================\n",
        "    # 📊 EVALUATION MODE SETTINGS (Only used when MODE = 'eval')\n",
        "    # =============================================================================\n",
        "\n",
        "    # Required for evaluation\n",
        "    QA_FILE = '/content/AutoRAG_ARAGOG_Paper/data/qa.parquet'  # QA pairs file\n",
        "    EVALUATOR_API_KEY = None  # Separate API key for evaluator (optional, uses API_KEY if None)\n",
        "\n",
        "    # Optional evaluation settings\n",
        "    RESUME_EVALUATION_FILE = None  # Resume from this evaluation file (optional)\n",
        "    RETRY_FAILED_EVALUATIONS = False  # Retry failed evaluations when resuming\n",
        "    START_EVALUATION_INDEX = 0  # Start evaluation from this question index\n",
        "\n",
        "    # =============================================================================\n",
        "    # 💬 APP MODE SETTINGS (Only used when MODE = 'app')\n",
        "    # =============================================================================\n",
        "\n",
        "    # Optional: Auto-extract PDFs (if None, skips extraction)\n",
        "    PDF_INPUT_DIR = 'data/pdfs'  # Directory with PDF files for auto-extraction\n",
        "    # Set to None to skip auto-extraction (folder with extracted pdfs already exists):\n",
        "    # PDF_INPUT_DIR = None\n",
        "\n",
        "    # Required for app mode\n",
        "    DATA_DIR = 'data/extracted'  # Directory containing extracted data\n",
        "\n",
        "    # PDF extraction settings (only used if PDF_INPUT_DIR is provided)\n",
        "    MAX_WORKERS = 3  # Parallel workers for PDF processing\n",
        "    EXTRACT_IMAGES = False  # Whether to extract images from PDFs\n",
        "\n",
        "    # =============================================================================\n",
        "    # 🚀 APPLICATION EXECUTION - DO NOT EDIT BELOW THIS LINE\n",
        "    # =============================================================================\n",
        "\n",
        "    print(\"🚀 RAG System Main Application\")\n",
        "    print(f\"Mode: {MODE}\")\n",
        "    print(f\"Configuration: {'custom' if CUSTOM_CONFIG else CONFIG_NAME}\")\n",
        "    print(f\"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Input validation\n",
        "    if not API_KEY or API_KEY == \"your_api_key_here\":\n",
        "        print(\"❌ Error: Please set your API_KEY in the main() function\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    app = RAGMainApplication()\n",
        "\n",
        "    try:\n",
        "        if MODE == 'eval':\n",
        "            print(f\"\\n📊 EVALUATION MODE\")\n",
        "            print(f\"   QA File: {QA_FILE}\")\n",
        "            if RESUME_EVALUATION_FILE:\n",
        "                print(f\"   Resume: {RESUME_EVALUATION_FILE}\")\n",
        "\n",
        "            success = app.run_evaluation_mode(\n",
        "                config_name=CONFIG_NAME,\n",
        "                custom_config=CUSTOM_CONFIG,\n",
        "                api_key=API_KEY,\n",
        "                qa_file=QA_FILE,\n",
        "                evaluator_api_key=EVALUATOR_API_KEY,\n",
        "                resume_file=RESUME_EVALUATION_FILE,\n",
        "                retry_failed=RETRY_FAILED_EVALUATIONS,\n",
        "                start_index=START_EVALUATION_INDEX\n",
        "            )\n",
        "\n",
        "        elif MODE == 'app':\n",
        "            print(f\"\\n💬 APP MODE\")\n",
        "            print(f\"   Data Dir: {DATA_DIR}\")\n",
        "            if PDF_INPUT_DIR:\n",
        "                print(f\"   PDF Input Dir: {PDF_INPUT_DIR} (auto-extract enabled)\")\n",
        "\n",
        "            success = app.run_app_mode(\n",
        "                config_name=CONFIG_NAME,\n",
        "                custom_config=CUSTOM_CONFIG,\n",
        "                api_key=API_KEY,\n",
        "                data_dir=DATA_DIR,\n",
        "                pdf_input_dir=PDF_INPUT_DIR,\n",
        "                max_workers=MAX_WORKERS,\n",
        "                extract_images=EXTRACT_IMAGES\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            print(f\"❌ Unknown mode: {MODE}\")\n",
        "            print(\"   Valid modes: 'eval', 'app'\")\n",
        "            success = False\n",
        "\n",
        "        if success:\n",
        "            print(f\"\\n✅ {MODE.upper()} mode completed successfully!\")\n",
        "        else:\n",
        "            print(f\"\\n❌ {MODE.upper()} mode failed!\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"\\n⏹️  {MODE.upper()} mode interrupted by user\")\n",
        "        sys.exit(0)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n💥 Unexpected error in {MODE.upper()} mode: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lUFWReY0FUXi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}